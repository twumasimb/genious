{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datasets\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed, broadcast_object_list\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizerFast,\n",
    "    BertForPreTraining,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")\n",
    "from transformers.utils.versions import require_version\n",
    "from selectionstrategies import SubmodStrategy\n",
    "from accelerate import InitProcessGroupKwargs\n",
    "from selectionstrategies.helper_fns import taylor_softmax_v1\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables \n",
    "dataset_name = \"wikitext\"\n",
    "dataset_config_name = \"wikitext-2-raw-v1\"\n",
    "validation_split_percentage = 80\n",
    "model_config_name = None\n",
    "tokenizer_name = \"bert-base-uncased\"\n",
    "use_slow_tokenizer = False  # Bool\n",
    "num_workers = None # (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and Preprocess the dataset for the task.\n",
    "raw_datasets = load_dataset(dataset_name, dataset_config_name)\n",
    "\n",
    "if 'validation' not in raw_datasets.keys():\n",
    "    raw_datasets=raw_datasets[\"train\"].train_test_split(test_size=(validation_split_percentage/100), shuffle=False)\n",
    "    raw_datasets=datasets.DatasetDict({\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and instance of the model along with its tokenizer\n",
    "\n",
    "# Load the model\n",
    "config = BertConfig.from_pretrained(model_config_name)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name, use_fast= not use_slow_tokenizer)\n",
    "\n",
    "# Instantiating the model\n",
    "model = BertForPretraining(tokenizer_name)\n",
    "\n",
    "# Resizing the token embeddings to fit the tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and group the data based on the kind of model\n",
    "\n",
    "column_names=raw_datasets['train'].column_names\n",
    "text_column_name=\"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name], return_special_token_mask=True)\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=num_workers, \n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on every text in dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datacollators and dataloaders \n",
    "\n",
    "# warmstart dataloader\n",
    "\n",
    "# first sent nsp zero dataloader\n",
    "\n",
    "# first sent nsp one  dataloader\n",
    "\n",
    "# subset dataloader (train)\n",
    "\n",
    "# eval dataloader (validation & testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and training instance\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "# learning scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmstart the model: Train the model with the warmstart data for warmstart epochs\n",
    "\n",
    "# Plot both training & perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subset selection strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin subset selection \n",
    "\n",
    "# Choose a selection strategy\n",
    "\n",
    "# Unwrap the model and set it in evaluation mode.\n",
    "\n",
    "# Go through the model specific dataset\n",
    "\n",
    "# Get the embedding and save them in a list.\n",
    "\n",
    "# Using the list and the selection strategy, get the indices and the gains of each data point in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the data into a dataset called subset_dataset\n",
    "\n",
    "# add the data to the subset dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with importance re-sampling\n",
    "\n",
    "# Train on the entire dataset once\n",
    "\n",
    "# Sample using the indices and gains\n",
    "\n",
    "# Train the model on the sampled dataset\n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "# Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personal Addition: Inference on the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingenious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
