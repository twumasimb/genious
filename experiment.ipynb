{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datasets\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed, broadcast_object_list\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizerFast,\n",
    "    BertForPreTraining,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")\n",
    "from transformers.utils.versions import require_version\n",
    "from selectionstrategies import SubmodStrategy\n",
    "from accelerate import InitProcessGroupKwargs\n",
    "from selectionstrategies.helper_fns import taylor_softmax_v1\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM']='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler\n",
    "file_handler = logging.FileHandler('logfile.log')\n",
    "\n",
    "# Create a formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set the formatter for the file handler\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the file handler to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Log a message\n",
    "logger.info('Logger created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables \n",
    "dataset_name = \"Salesforce/wikitext\"\n",
    "dataset_config_name = \"wikitext-2-raw-v1\"\n",
    "validation_split_percentage = 80\n",
    "model_config_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer_name = \"bert-base-uncased\"\n",
    "use_slow_tokenizer = False  # Bool\n",
    "num_workers = None # (int)\n",
    "max_seq_len = 128\n",
    "short_seq_prob = 0.1\n",
    "nsp_probability = 0.1\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and Preprocess the dataset for the task.\n",
    "raw_datasets = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "if 'validation' not in raw_datasets.keys():\n",
    "    raw_datasets=raw_datasets[\"train\"].train_test_split(test_size=(validation_split_percentage/100), shuffle=False)\n",
    "    raw_datasets=datasets.DatasetDict({\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to use custom config\n",
    "\n",
    "# config = BertConfig(\n",
    "#     vocab_size=vocab_size,\n",
    "#     hidden_size=hidden_size,\n",
    "#     num_hidden_layers=num_hidden_layers,\n",
    "#     num_attention_heads=num_attention_heads,\n",
    "#     intermediate_size=intermediate_size,\n",
    "#     hidden_act=\"gelu\",\n",
    "#     hidden_dropout_prob=0.1,\n",
    "#     attention_probs_dropout_prob=0.1,\n",
    "#     max_position_embeddings=512,\n",
    "#     type_vocab_size=2,\n",
    "#     initializer_range=0.02,\n",
    "#     layer_norm_eps=1e-12,\n",
    "#     position_embedding_type=\"absolute\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and instance of the model along with its tokenizer\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name, use_fast= not use_slow_tokenizer)\n",
    "\n",
    "# Load the model\n",
    "config = BertConfig.from_pretrained(model_config_name)\n",
    "\n",
    "# Instantiating the model\n",
    "model = BertForPreTraining(config)\n",
    "\n",
    "# Resizing the token embeddings to fit the tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33d47c124fb4b16bac0239c484cea43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on every text in dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018e15b8b90d44eca9c2a987df0154af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on every text in dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f72ada209db49d18b3604db127e430d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on every text in dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc09c1bb64040a7801b64cd08fcdc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping Train texts into chucks of 128:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba97f039b3f34a2186295e844aa87707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping Validation texts into chucks of 128:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize and group the data based on the kind of model\n",
    "\n",
    "column_names=raw_datasets['train'].column_names\n",
    "text_column_name=\"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_dataset = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=num_workers, \n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on every text in dataset\"\n",
    ")\n",
    "\n",
    "# Grouping the data \n",
    "from experiment_utils import group_texts\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "eval_dataset = tokenized_dataset[\"validation\"]\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    group_texts, \n",
    "    fn_kwargs={'split': 'train', 'tokenizer':tokenizer, 'max_seq_length': max_seq_len, \n",
    "               'short_seq_prob':short_seq_prob, 'nsp_probability':nsp_probability, 'tokenized_datasets':tokenized_dataset},\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=num_workers,\n",
    "    with_indices=True,\n",
    "    desc=f\"Grouping Train texts into chucks of {max_seq_len}\"\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    group_texts, \n",
    "    fn_kwargs={'split': 'validation', 'tokenizer':tokenizer, 'max_seq_length': max_seq_len, \n",
    "               'short_seq_prob':short_seq_prob, 'nsp_probability':nsp_probability, 'tokenized_datasets':tokenized_dataset},\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=num_workers,\n",
    "    with_indices=True,\n",
    "    desc=f\"Grouping Validation texts into chucks of {max_seq_len}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'next_sentence_label'],\n",
       "    num_rows: 1204\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] using lobster pots, although lines baited with octopus or cuttlefish sometimes succeed in tempting them out, to allow them to be caught in a net or by hand. in 2008, 4 @, @ 386 t of h. gammarus were caught across europe and north africa, of which 3 @, @ 462 t ( 79 % ) was caught in the british isles ( including the channel islands ). the minimum landing size for h. gammarus is a carapace length of 87 mm ( 3 @. @ [SEP] aquaculture systems for h. gammarus are under development, and production rates are still very low. [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(eval_dataset[9]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcad3989db441de8429eb04cf193b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0448146e81c0488f92a0388e6843034e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43081df1c9104029a549e3b994bc04db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12de0bf59a84e03b97c868fe156d3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5341 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the data\n",
    "prepared_data = datasets.DatasetDict({\"train\": train_dataset, \"validation\": eval_dataset})\n",
    "dataset=prepared_data['train']\n",
    "\n",
    "def extract_first_sentences(examples):\n",
    "    for i, input_ids in enumerate(examples[\"input_ids\"]):\n",
    "        idx=input_ids.index(tokenizer.sep_token_id)\n",
    "        examples[\"input_ids\"][i]=input_ids[:idx+1]\n",
    "        examples[\"attention_mask\"][i]=examples[\"attention_mask\"][i][:idx+1]\n",
    "        examples[\"token_type_ids\"][i]=examples[\"token_type_ids\"][i][:idx+1]\n",
    "        examples[\"special_tokens_mask\"][i]=examples[\"special_tokens_mask\"][i][:idx+1]\n",
    "    return examples\n",
    "\n",
    "# Separate the data into those that have the next sentence labels and those that do not.\n",
    "nsp_zero=dataset.filter(lambda examples: [x==0 for x in examples[\"next_sentence_label\"]], batched=True, num_proc=num_workers, keep_in_memory=True)\n",
    "nsp_one=dataset.filter(lambda examples: [x==1 for x in examples[\"next_sentence_label\"]], batched=True, num_proc=num_workers, keep_in_memory=True)\n",
    "\n",
    "# Extract the first sentences from both datasets\n",
    "first_sent_nsp_zero=nsp_zero.map(extract_first_sentences, batched=True, num_proc=num_workers, remove_columns=[\"next_sentence_label\", \"special_tokens_mask\"], keep_in_memory=True)\n",
    "first_sent_nsp_one=nsp_one.map(extract_first_sentences, batched=True, num_proc=num_workers, remove_columns=[\"next_sentence_label\", \"special_tokens_mask\"], keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] movement limited by their action gauge. up to nine characters can be assigned to a single mission. during gameplay, characters will call out if something happens to them, such as their health points ( hp ) getting low or being knocked out by enemy attacks. each character has specific \" potentials \", skills unique to each character. they are divided into \" personal potential \", which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character, and \" battle potentials \", which are grown throughout the game and always grant boons to a character. to learn battle potential [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(first_sent_nsp_one[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_fraction = 0.25\n",
    "\n",
    "# Sample a subset of the train dataset\n",
    "num_samples = int(round(len(train_dataset) * subset_fraction, 0))\n",
    "init_subset_indices = [random.sample(list(range(len(train_dataset))), num_samples)]\n",
    "full_dataset=train_dataset\n",
    "subset_dataset = full_dataset.select(init_subset_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 6528\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sent_nsp_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'next_sentence_label'],\n",
       "    num_rows: 11869\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'next_sentence_label'],\n",
       "    num_rows: 2967\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_probability = 0.15\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 4\n",
    "\n",
    "# Create datacollators\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "data_collator_embd = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# warmstart dataloader (train on all the train dataset during warmup)\n",
    "warmstart_dataloader = DataLoader(train_dataset.remove_columns(['special_tokens_mask']), shuffle=True, collate_fn=data_collator, batch_size=train_batch_size)\n",
    "\n",
    "# first sent nsp zero dataloader\n",
    "first_sent_nsp_zero_dataloader=DataLoader(first_sent_nsp_zero, shuffle=False, collate_fn=data_collator_embd, batch_size=eval_batch_size)\n",
    "\n",
    "# first sent nsp one  dataloader\n",
    "first_sent_nsp_one_dataloader=DataLoader(first_sent_nsp_one, shuffle=False, collate_fn=data_collator_embd, batch_size=eval_batch_size)\n",
    "\n",
    "# subset dataloader (train)\n",
    "subset_dataloader=DataLoader(subset_dataset.remove_columns(['special_tokens_mask']), shuffle=True, collate_fn=data_collator, batch_size=train_batch_size,)\n",
    "\n",
    "# eval dataloader (validation & testing)\n",
    "eval_dataloader=DataLoader(eval_dataset.remove_columns(['special_tokens_mask']), collate_fn=data_collator, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "scheduler_name = 'linear'\n",
    "num_warmup_steps = 100\n",
    "num_training_steps = 100\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# learning scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=scheduler_name,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmer = iter(eval_dataloader)\n",
    "X= next(warmer)\n",
    "X['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining warmstart\n",
      "Completed Steps: 1; Loss: 11.13366413116455; lr: [0.0];\n",
      "Completed Steps: 2; Loss: 11.095403671264648; lr: [1.0000000000000002e-06];\n",
      "Completed Steps: 3; Loss: 11.291232109069824; lr: [2.0000000000000003e-06];\n",
      "Completed Steps: 4; Loss: 11.166280746459961; lr: [3e-06];\n",
      "Completed Steps: 5; Loss: 11.307530403137207; lr: [4.000000000000001e-06];\n",
      "Completed Steps: 6; Loss: 11.00561809539795; lr: [5e-06];\n",
      "Completed Steps: 7; Loss: 11.058542251586914; lr: [6e-06];\n",
      "Completed Steps: 8; Loss: 10.953042984008789; lr: [7.000000000000001e-06];\n",
      "Completed Steps: 9; Loss: 11.167601585388184; lr: [8.000000000000001e-06];\n",
      "Completed Steps: 10; Loss: 10.858426094055176; lr: [9e-06];\n",
      "Completed Steps: 11; Loss: 10.713296890258789; lr: [1e-05];\n",
      "Completed Steps: 12; Loss: 10.525032043457031; lr: [1.1000000000000001e-05];\n",
      "Completed Steps: 13; Loss: 10.447954177856445; lr: [1.2e-05];\n",
      "Completed Steps: 14; Loss: 10.478153228759766; lr: [1.3000000000000001e-05];\n",
      "Completed Steps: 15; Loss: 11.15766716003418; lr: [1.4000000000000001e-05];\n",
      "Completed Steps: 16; Loss: 10.309829711914062; lr: [1.5e-05];\n",
      "Completed Steps: 17; Loss: 10.299623489379883; lr: [1.6000000000000003e-05];\n",
      "Completed Steps: 18; Loss: 10.317249298095703; lr: [1.7000000000000003e-05];\n",
      "Completed Steps: 19; Loss: 9.87266731262207; lr: [1.8e-05];\n",
      "Completed Steps: 20; Loss: 10.151647567749023; lr: [1.9e-05];\n",
      "Completed Steps: 21; Loss: 10.27202320098877; lr: [2e-05];\n",
      "Completed Steps: 22; Loss: 10.148334503173828; lr: [2.1e-05];\n",
      "Completed Steps: 23; Loss: 9.830850601196289; lr: [2.2000000000000003e-05];\n",
      "Completed Steps: 24; Loss: 10.613033294677734; lr: [2.3000000000000003e-05];\n",
      "Completed Steps: 25; Loss: 9.783895492553711; lr: [2.4e-05];\n",
      "Completed Steps: 26; Loss: 9.253010749816895; lr: [2.5e-05];\n",
      "Completed Steps: 27; Loss: 9.607565879821777; lr: [2.6000000000000002e-05];\n",
      "Completed Steps: 28; Loss: 9.901239395141602; lr: [2.7000000000000002e-05];\n",
      "Completed Steps: 29; Loss: 9.773160934448242; lr: [2.8000000000000003e-05];\n",
      "Completed Steps: 30; Loss: 10.863300323486328; lr: [2.9e-05];\n",
      "Completed Steps: 31; Loss: 9.923995971679688; lr: [3e-05];\n",
      "Completed Steps: 32; Loss: 9.703719139099121; lr: [3.1e-05];\n",
      "Completed Steps: 33; Loss: 9.667468070983887; lr: [3.2000000000000005e-05];\n",
      "Completed Steps: 34; Loss: 9.834696769714355; lr: [3.3e-05];\n",
      "Completed Steps: 35; Loss: 9.267909049987793; lr: [3.4000000000000007e-05];\n",
      "Completed Steps: 36; Loss: 9.840989112854004; lr: [3.5e-05];\n",
      "Completed Steps: 37; Loss: 9.356724739074707; lr: [3.6e-05];\n",
      "Completed Steps: 38; Loss: 9.578278541564941; lr: [3.7e-05];\n",
      "Completed Steps: 39; Loss: 10.048948287963867; lr: [3.8e-05];\n",
      "Completed Steps: 40; Loss: 9.938665390014648; lr: [3.9000000000000006e-05];\n",
      "Completed Steps: 41; Loss: 10.101746559143066; lr: [4e-05];\n",
      "Completed Steps: 42; Loss: 9.041081428527832; lr: [4.1e-05];\n",
      "Completed Steps: 43; Loss: 9.67872428894043; lr: [4.2e-05];\n",
      "Completed Steps: 44; Loss: 9.228104591369629; lr: [4.3e-05];\n",
      "Completed Steps: 45; Loss: 9.435218811035156; lr: [4.4000000000000006e-05];\n",
      "Completed Steps: 46; Loss: 9.27138614654541; lr: [4.5e-05];\n",
      "Completed Steps: 47; Loss: 8.836596488952637; lr: [4.600000000000001e-05];\n",
      "Completed Steps: 48; Loss: 8.985424995422363; lr: [4.7e-05];\n",
      "Completed Steps: 49; Loss: 9.722946166992188; lr: [4.8e-05];\n",
      "Completed Steps: 50; Loss: 9.386923789978027; lr: [4.9e-05];\n",
      "Completed Steps: 51; Loss: 9.422243118286133; lr: [5e-05];\n",
      "Completed Steps: 52; Loss: 8.928674697875977; lr: [5.1000000000000006e-05];\n",
      "Completed Steps: 53; Loss: 9.24572467803955; lr: [5.2000000000000004e-05];\n",
      "Completed Steps: 54; Loss: 9.416597366333008; lr: [5.300000000000001e-05];\n",
      "Completed Steps: 55; Loss: 9.811005592346191; lr: [5.4000000000000005e-05];\n",
      "Completed Steps: 56; Loss: 8.618721961975098; lr: [5.500000000000001e-05];\n",
      "Completed Steps: 57; Loss: 8.994023323059082; lr: [5.6000000000000006e-05];\n",
      "Completed Steps: 58; Loss: 8.971879005432129; lr: [5.6999999999999996e-05];\n",
      "Completed Steps: 59; Loss: 9.690677642822266; lr: [5.8e-05];\n",
      "Completed Steps: 60; Loss: 8.40870475769043; lr: [5.9e-05];\n",
      "Completed Steps: 61; Loss: 8.347789764404297; lr: [6e-05];\n",
      "Completed Steps: 62; Loss: 10.019388198852539; lr: [6.1e-05];\n",
      "Completed Steps: 63; Loss: 8.094961166381836; lr: [6.2e-05];\n",
      "Completed Steps: 64; Loss: 9.383878707885742; lr: [6.3e-05];\n",
      "Completed Steps: 65; Loss: 8.912179946899414; lr: [6.400000000000001e-05];\n",
      "Completed Steps: 66; Loss: 8.775952339172363; lr: [6.500000000000001e-05];\n",
      "Completed Steps: 67; Loss: 8.570012092590332; lr: [6.6e-05];\n",
      "Completed Steps: 68; Loss: 9.021344184875488; lr: [6.7e-05];\n",
      "Completed Steps: 69; Loss: 9.052098274230957; lr: [6.800000000000001e-05];\n",
      "Completed Steps: 70; Loss: 8.899551391601562; lr: [6.9e-05];\n",
      "Completed Steps: 71; Loss: 8.54536247253418; lr: [7e-05];\n",
      "Completed Steps: 72; Loss: 8.859891891479492; lr: [7.1e-05];\n",
      "Completed Steps: 73; Loss: 9.481819152832031; lr: [7.2e-05];\n",
      "Completed Steps: 74; Loss: 9.089218139648438; lr: [7.3e-05];\n",
      "Completed Steps: 75; Loss: 8.654525756835938; lr: [7.4e-05];\n",
      "Completed Steps: 76; Loss: 7.995271682739258; lr: [7.500000000000001e-05];\n",
      "Completed Steps: 77; Loss: 8.678248405456543; lr: [7.6e-05];\n",
      "Completed Steps: 78; Loss: 9.013540267944336; lr: [7.7e-05];\n",
      "Completed Steps: 79; Loss: 9.08248233795166; lr: [7.800000000000001e-05];\n",
      "Completed Steps: 80; Loss: 7.814805030822754; lr: [7.900000000000001e-05];\n",
      "Completed Steps: 81; Loss: 9.079485893249512; lr: [8e-05];\n",
      "Completed Steps: 82; Loss: 8.648574829101562; lr: [8.1e-05];\n",
      "Completed Steps: 83; Loss: 8.20637035369873; lr: [8.2e-05];\n",
      "Completed Steps: 84; Loss: 8.330599784851074; lr: [8.3e-05];\n",
      "Completed Steps: 85; Loss: 8.05349063873291; lr: [8.4e-05];\n",
      "Completed Steps: 86; Loss: 8.032785415649414; lr: [8.5e-05];\n",
      "Completed Steps: 87; Loss: 8.619539260864258; lr: [8.6e-05];\n",
      "Completed Steps: 88; Loss: 8.170392036437988; lr: [8.7e-05];\n",
      "Completed Steps: 89; Loss: 8.231499671936035; lr: [8.800000000000001e-05];\n",
      "Completed Steps: 90; Loss: 7.632972717285156; lr: [8.900000000000001e-05];\n",
      "Completed Steps: 91; Loss: 8.783788681030273; lr: [9e-05];\n",
      "Completed Steps: 92; Loss: 8.824713706970215; lr: [9.1e-05];\n",
      "Completed Steps: 93; Loss: 7.8847270011901855; lr: [9.200000000000001e-05];\n",
      "Completed Steps: 94; Loss: 8.590706825256348; lr: [9.300000000000001e-05];\n",
      "Completed Steps: 95; Loss: 7.573122978210449; lr: [9.4e-05];\n",
      "Completed Steps: 96; Loss: 8.1697359085083; lr: [9.5e-05];\n",
      "Completed Steps: 97; Loss: 8.588438987731934; lr: [9.6e-05];\n",
      "Completed Steps: 98; Loss: 8.81187629699707; lr: [9.7e-05];\n",
      "Completed Steps: 99; Loss: 7.894064903259277; lr: [9.8e-05];\n",
      "Completed Steps: 100; Loss: 7.874598026275635; lr: [9.900000000000001e-05];\n",
      "Epoch 1: Perplexity: 4087.8519586998873\n"
     ]
    }
   ],
   "source": [
    "warmstart_epochs = 1\n",
    "completed_steps = 0\n",
    "\n",
    "# Warmstart the model: Train the model with the warmstart data for warmstart epochs\n",
    "for epoch in range(warmstart_epochs):\n",
    "    if epoch==0:\n",
    "        print(\"Begining warmstart\")\n",
    "    model.train() # Setup the model for training\n",
    "    for step, batch in enumerate(warmstart_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        print(f\"Completed Steps: {1+completed_steps}; Loss: {loss.detach().float()}; lr: {lr_scheduler.get_last_lr()};\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "        if completed_steps >= num_warmup_steps:\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():  # Use torch.no_grad() instead of inference_mode()\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.view(1))  # Add the loss as a 1-dimensional tensor\n",
    "\n",
    "    if losses:\n",
    "        losses = torch.cat(losses)\n",
    "        losses = losses[:len(eval_dataset)]\n",
    "        try:\n",
    "            perplexity = math.exp(torch.mean(losses).item())\n",
    "        except OverflowError:\n",
    "            perplexity = float(\"inf\")\n",
    "    else:\n",
    "        perplexity = float(\"inf\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Perplexity: {perplexity}\")\n",
    "\n",
    "# Plot both training & perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_strategy = 'fl'\n",
    "num_partitions = 200 # Default is 5000\n",
    "partition_strategy = 'random'\n",
    "submod_optimizer = 'LazyGreedy'\n",
    "\n",
    "# Define subset selection strategies\n",
    "\n",
    "subset_strategy = SubmodStrategy(logger, selection_strategy,\n",
    "    num_partitions=num_partitions, partition_strategy=partition_strategy,\n",
    "    optimizer=submod_optimizer, similarity_criterion='feature', \n",
    "    metric='cosine', eta=1, stopIfZeroGain=False, \n",
    "    stopIfNegativeGain=False, verbose=False, lambdaVal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representations(NSP Class 0) Size: 38.32475280761719, Total number of samples: 6528\n",
      "Length of indices: 6528\n",
      "Representations(NSP Class 0) gathered. Shape of representations: (6528, 768). Length of indices: 6528\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "27\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 99%|█████████▉| 198/200 [00:00<00:00, 1192.79it/s]% [Iteration  [Iteration 3232 of  of 3232]]302 of  of 3232]]2]2]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 768)) while a minimum of 1 is required by check_pairwise_arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/twumasimb/miniconda3/envs/ingenious/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/twumasimb/Projects/genious/selectionstrategies/submodstrategy.py\", line 17, in partition_subset_strat\n    return partition_subset_selection(*args)\n  File \"/home/twumasimb/Projects/genious/selectionstrategies/submodstrategy.py\", line 21, in partition_subset_selection\n    data_sijs=submodlib.helper.create_kernel(X=representations, metric=metric, method=\"sklearn\")\n  File \"/home/twumasimb/miniconda3/envs/ingenious/lib/python3.9/site-packages/submodlib/helper.py\", line 521, in create_kernel\n    dense = globals()['create_kernel_dense_'+method](X, metric, X_rep)\n  File \"/home/twumasimb/miniconda3/envs/ingenious/lib/python3.9/site-packages/submodlib/helper.py\", line 279, in create_kernel_dense_sklearn\n    dense = cosine_similarity(X)\n  File \"/home/twumasimb/miniconda3/envs/ingenious/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/twumasimb/miniconda3/envs/ingenious/lib/python3.9/site-packages/sklearn/metrics/pairwise.py\", line 1657, in cosine_similarity\n    X, Y = check_pairwise_arrays(X, Y)\n  File \"/home/twumasimb/miniconda3/envs/ingenious/lib/python3.9/site-packages/sklearn/metrics/pairwise.py\", line 155, in check_pairwise_arrays\n    X = Y = check_array(\n  File \"/home/twumasimb/miniconda3/envs/ingenious/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1072, in check_array\n    raise ValueError(\nValueError: Found array with 0 sample(s) (shape=(0, 768)) while a minimum of 1 is required by check_pairwise_arrays.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength of indices: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(batch_indices_nsp_zero)))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRepresentations(NSP Class 0) gathered. Shape of representations: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Length of indices: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(representations_nsp_zero\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mlen\u001b[39m(batch_indices_nsp_zero)))\n\u001b[0;32m---> 48\u001b[0m partition_indices_nsp_zero, greedyIdx_nsp_zero, gains_nsp_zero \u001b[38;5;241m=\u001b[39m \u001b[43msubset_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_indices_nsp_zero\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                                                                        \u001b[49m\u001b[43mbatch_indices_nsp_zero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentations_nsp_zero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                                                                                        \u001b[49m\u001b[43mparallel_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_gains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m subset_indices_nsp_zero \u001b[38;5;241m=\u001b[39m [[]]\n\u001b[1;32m     52\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Projects/genious/selectionstrategies/submodstrategy.py:135\u001b[0m, in \u001b[0;36mSubmodStrategy.select\u001b[0;34m(self, budget, indices, representations, parallel_processes, return_gains)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Parallel computation of subsets\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(parallel_processes) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 135\u001b[0m     greedyIdx_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition_subset_strat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mquery_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepresentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_budgets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmi_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_gains\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpartition_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_gains:\n\u001b[1;32m    139\u001b[0m     gains\u001b[38;5;241m=\u001b[39m[p[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m greedyIdx_list]\n",
      "File \u001b[0;32m~/miniconda3/envs/ingenious/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ingenious/lib/python3.9/multiprocessing/pool.py:870\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 768)) while a minimum of 1 is required by check_pairwise_arrays."
     ]
    }
   ],
   "source": [
    "selection_strategy = 'fl'\n",
    "layer_for_similarity_computation = 9\n",
    "temperature = 0.5\n",
    "seed = 23\n",
    "parallel_processes = None\n",
    "# num_samples has already been defined when creating subset.\n",
    "probs_nsp_zero=[]\n",
    "probs_nsp_one=[]\n",
    "greedyList_nsp_zero=[]\n",
    "greedyList_nsp_one=[]\n",
    "gains_nsp_zero=[]\n",
    "gains_nsp_one=[]\n",
    "\n",
    "# Begin subset selection \n",
    "if selection_strategy == 'Random-Online':\n",
    "    subset_indices_nsp_zero = [random.sample(list(range(len(first_sent_nsp_zero))), math.floor(num_samples/2))]\n",
    "    subset_indices_nsp_one = [random.sample(list(range(len(first_sent_nsp_one))), math.ceil(num_samples/2))]\n",
    "elif selection_strategy in ['fl', 'logdet', 'gc', 'disparity-sum']:\n",
    "    # Choose a selection strategy\n",
    "    model.eval() # Set the model in evaluation model \n",
    "    representations_nsp_zero=[]\n",
    "    batch_indices_nsp_zero=[]\n",
    "    total_cnt=0\n",
    "    total_storage=0\n",
    "    # Unwrap the model and set it in evaluation mode.\n",
    "    for step, batch in enumerate(first_sent_nsp_zero_dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = model(**batch, output_hidden_states=True)\n",
    "        embeddings=output[\"hidden_states\"][layer_for_similarity_computation]\n",
    "        mask=(batch['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float())\n",
    "        mask1=((batch['token_type_ids'].unsqueeze(-1).expand(embeddings.size()).float())==0)\n",
    "        mask=mask*mask1\n",
    "        mean_pooled=torch.sum(embeddings*mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "        total_cnt += mean_pooled.size(0)\n",
    "        mean_pooled = mean_pooled.cpu()\n",
    "        total_storage += sys.getsizeof(mean_pooled.storage())\n",
    "        representations_nsp_zero.append(mean_pooled)\n",
    "\n",
    "    representations_nsp_zero=torch.cat(representations_nsp_zero, dim=0)\n",
    "    representations_nsp_zero=representations_nsp_zero[:len(first_sent_nsp_zero)]\n",
    "    total_storage += sys.getsizeof(representations_nsp_zero.storage())\n",
    "    representations_nsp_zero=representations_nsp_zero.numpy()\n",
    "    print('Representations(NSP Class 0) Size: {}, Total number of samples: {}'.format(total_storage/(1024 * 1024), total_cnt))\n",
    "    batch_indices_nsp_zero=list(range(len(first_sent_nsp_zero)))\n",
    "    print('Length of indices: {}'.format(len(batch_indices_nsp_zero)))\n",
    "    print('Representations(NSP Class 0) gathered. Shape of representations: {}. Length of indices: {}'.format(representations_nsp_zero.shape, len(batch_indices_nsp_zero)))\n",
    "\n",
    "    partition_indices_nsp_zero, greedyIdx_nsp_zero, gains_nsp_zero = subset_strategy.select(len(batch_indices_nsp_zero)-1, \n",
    "                                                                                            batch_indices_nsp_zero, representations_nsp_zero, \n",
    "                                                                                            parallel_processes=parallel_processes, return_gains=True)\n",
    "    subset_indices_nsp_zero = [[]]\n",
    "    i=0\n",
    "    for p in gains_nsp_zero:\n",
    "        greedyList_nsp_zero.append(greedyIdx_nsp_zero[i:i+len(p)])         \n",
    "        i+=len(p)\n",
    "    probs_nsp_zero=[taylor_softmax_v1(torch.from_numpy(np.array([partition_gains])/temperature)).numpy()[0] for partition_gains in gains_nsp_zero]\n",
    "    rng=np.random.default_rng(seed+completed_steps)\n",
    "    for i, partition_prob in enumerate(probs_nsp_zero):\n",
    "        partition_budget=min(math.ceil((len(partition_prob)/len(batch_indices_nsp_zero)) * math.floor(num_samples/2)), len(partition_prob)-1)\n",
    "        subset_indices_nsp_zero[0].extend(rng.choice(greedyList_nsp_zero[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "# Go through the model specific dataset\n",
    "\n",
    "# Get the embedding and save them in a list.\n",
    "\n",
    "# Using the list and the selection strategy, get the indices and the gains of each data point in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the data into a dataset called subset_dataset\n",
    "\n",
    "# add the data to the subset dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with importance re-sampling\n",
    "\n",
    "# Train on the entire dataset once\n",
    "\n",
    "# Sample using the indices and gains\n",
    "\n",
    "# Train the model on the sampled dataset\n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "# Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personal Addition: Inference on the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingenious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
