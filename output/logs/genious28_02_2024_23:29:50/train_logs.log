02/28/2024 23:29:55 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

02/28/2024 23:31:16 - INFO - __main__ - Data is not preprocessed.
02/28/2024 23:31:16 - INFO - __main__ - Loading the data.
02/28/2024 23:33:49 - INFO - __main__ - Data is not preprocessed.
02/28/2024 23:33:49 - INFO - __main__ - Loading the data.
02/28/2024 23:33:50 - WARNING - datasets.builder - Reusing dataset bookcorpus (/home/twumasimb/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)
02/28/2024 23:34:44 - INFO - __main__ - Loading the model configuration.
02/28/2024 23:34:44 - INFO - __main__ - Loading the tokenizer.
02/28/2024 23:34:45 - INFO - __main__ - Initializing Model.
02/28/2024 23:34:45 - INFO - __main__ - Training a new model from scratch
02/28/2024 23:36:17 - INFO - __main__ - Beginning Tokenization.
02/28/2024 23:37:22 - INFO - __main__ - Beginning Tokenization.
02/28/2024 23:37:33 - WARNING - datasets.fingerprint - Parameter 'function'=<function tokenize_function at 0x7f97a9b08280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
02/28/2024 23:51:53 - INFO - __main__ - Grouping the tokenized dataset into chunks of 128.
