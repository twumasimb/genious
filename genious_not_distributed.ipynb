{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twumasimb/miniconda3/envs/ingenious/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datasets\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "# from accelerate import Accelerator, DistributedType\n",
    "# from accelerate.logging import get_logger\n",
    "# from accelerate.utils import set_seed, broadcast_object_list\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizerFast,\n",
    "    BertForPreTraining,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from selectionstrategies import SubmodStrategy\n",
    "# from accelerate import InitProcessGroupKwargs\n",
    "from selectionstrategies.helper_fns import taylor_softmax_v1\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Parameters for args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.log_dir = \"./path/to/log_dir\"  # Update with your path\n",
    "        self.subset_dir = \"./path/to/subset_dir\"  # Update with your path\n",
    "        self.output_dir = \"./path/to/output_dir\"  # Update with your path\n",
    "        self.preprocessed = False  # Set to True if preprocessed\n",
    "        self.load_data_from_disk = None  # Set to True if loading data from disk\n",
    "        self.data_directory = None  # Update with your path if load_data_from_disk is True\n",
    "        self.dataset_name = None  # Specify dataset name if needed\n",
    "        self.dataset_config_name = None  # Specify dataset config name if needed\n",
    "        self.train_file = None  # Path to your training file if applicable\n",
    "        self.validation_file = None  # Path to your validation file if applicable\n",
    "        self.validation_split_percentage = 5  # Update this as per your requirement\n",
    "        self.pad_to_max_length = False  # Set to True to pad to max_length\n",
    "        self.model_name_or_path = None  # Update with your model name or path\n",
    "        self.config_name = None  # Specify config name or path if not the same as model_name\n",
    "        self.hidden_size = 768\n",
    "        self.num_hidden_layers = 12\n",
    "        self.num_attention_heads = 12\n",
    "        self.intermediate_size = 3072\n",
    "        # Specify tokenizer name or path if not the same as model_name\n",
    "        self.tokenizer_name = None\n",
    "        self.vocab_size = 30522\n",
    "        self.use_slow_tokenizer = False  # Set to True to use a slow tokenizer\n",
    "        self.per_device_train_batch_size = 8\n",
    "        self.per_device_eval_batch_size = 8\n",
    "        self.learning_rate = 1e-4\n",
    "        self.lr_max_steps = 1000000\n",
    "        self.weight_decay = 0.01\n",
    "        self.max_train_steps = 250000\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.lr_scheduler_type = \"linear\"  # Adjust as needed, ensure it's a valid choice\n",
    "        self.num_warmup_steps = 10000\n",
    "        self.seed = None  # Specify a seed for reproducible training if needed\n",
    "        self.max_seq_length = 128\n",
    "        self.line_by_line = False  # Set to True if handling distinct lines as sequences\n",
    "        self.preprocessing_num_workers = 96\n",
    "        # Specify batch size during preprocessing if needed\n",
    "        self.preprocess_batch_size = None\n",
    "        self.overwrite_cache = False  # Set to True to overwrite cached sets\n",
    "        self.mlm_probability = 0.15\n",
    "        self.short_seq_prob = 0.1\n",
    "        self.nsp_probability = 0.5\n",
    "        self.subset_fraction = 0.25\n",
    "        self.selection_strategy = 'fl'  # Update with your strategy\n",
    "        self.optimizer = \"LazyGreedy\"  # Update with your optimizer\n",
    "        self.select_every = 25000\n",
    "        self.partition_strategy = \"random\"  # Update with your strategy\n",
    "        self.layer_for_similarity_computation = 9\n",
    "        self.num_partitions = 5000\n",
    "        self.parallel_processes = 96\n",
    "        self.num_warmstart_epochs = 10\n",
    "        self.checkpointing_steps = None  # Specify if needed\n",
    "        self.resume_from_checkpoint = None  # Specify path if resuming from checkpoint\n",
    "        self.temperature = 1.0  # Update if using a different temperature\n",
    "        self.main_process_port = None  # Update with your port\n",
    "        self.visible_gpus = '0'  # Update with your GPU(s)\n",
    "        # Specify to use a smaller portion of the data. Example: 20 means 80% of the data is used\n",
    "        self.init_split = None\n",
    "\n",
    "\n",
    "# Instantiate the args\n",
    "args = Args()\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "\n",
    "# The following can be modified as needed based on your requirements\n",
    "args.preprocessed = True\n",
    "args.visible_gpus = '1'  # Assuming you want to use the first GPU; adjust as needed\n",
    "args.main_process_port = 29500  # Example port; adjust as needed\n",
    "args.log_dir = f\"./output/logs/genious_{timestamp}/\"\n",
    "args.model_dir = f\"./output/models/genious_{timestamp}/\"\n",
    "args.subset_dir = f\"./output/subsets/genious_{timestamp}/\"\n",
    "args.load_data_from_disk = None\n",
    "args.data_directory = \"./data\"\n",
    "args.tokenizer_name = \"bert-base-uncased\"\n",
    "args.preprocess_batch_size = 100\n",
    "args.per_device_train_batch_size = 4\n",
    "args.per_device_eval_batch_size = 4\n",
    "args.learning_rate = 1e-4\n",
    "args.lr_max_steps = 1000\n",
    "args.weight_decay = 0.01\n",
    "args.max_train_steps = 10\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.num_warmup_steps = 10\n",
    "args.output_dir = args.model_dir\n",
    "args.model_type = \"bert\"\n",
    "args.max_seq_length = 64\n",
    "args.preprocessing_num_workers = 10\n",
    "args.mlm_probability = 0.15\n",
    "args.short_seq_prob = 0.1\n",
    "args.nsp_probability = 0.5\n",
    "args.subset_fraction = 0.25\n",
    "args.update_losses_every = 1250\n",
    "args.checkpointing_steps = 250\n",
    "args.init_split = 95  # ignore 95% of the data\n",
    "args.dataset_name = \"bookcorpus\"\n",
    "args.seed = 42\n",
    "\n",
    "\n",
    "# Set CUDA visible devices\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "os.makedirs(args.model_dir, exist_ok=True)\n",
    "os.makedirs(args.subset_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Logger & Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(args.log_dir, \"train_logs.log\"),\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# Ensure CUDA device is available and set as default for all tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(\n",
    "    torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    random.seed(args.seed)\n",
    "\n",
    "# Directly create the output directory if specified\n",
    "if args.output_dir is not None:\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "# Logging information about preprocessing\n",
    "if not args.preprocessed:\n",
    "    logger.info(\"Data is not preprocessed.\")\n",
    "    logger.info(\"Loading the data.\")\n",
    "    # Data loading logic remains the same\n",
    "    if args.load_data_from_disk is not None:\n",
    "        if args.data_directory is not None:\n",
    "            raw_datasets = load_from_disk(args.data_directory)\n",
    "            if \"validation\" not in raw_datasets.keys():\n",
    "                raw_datasets = raw_datasets[\"train\"].train_test_split(\n",
    "                    test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "                raw_datasets = datasets.DatasetDict(\n",
    "                    {\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})\n",
    "    elif args.dataset_name is not None:\n",
    "        raw_datasets = load_dataset(\n",
    "            args.dataset_name, args.dataset_config_name)\n",
    "        if \"validaton\" not in raw_datasets.keys() and args.init_split is not None:  # Use a smaller portion of the data\n",
    "            raw_datasets = raw_datasets[\"train\"].train_test_split(\n",
    "                test_size=(args.init_split/100), shuffle=False)\n",
    "            raw_datasets = raw_datasets[\"train\"].train_test_split(\n",
    "                test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "            raw_datasets = datasets.DatasetDict(\n",
    "                {\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})\n",
    "        elif \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets = raw_datasets.train_test_split(test_size=(\n",
    "                args.validation_split_percentage/100), shuffle=False)\n",
    "            raw_datasets = datasets.DatasetDict(\n",
    "                {\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})\n",
    "    else:\n",
    "        data_files = {}\n",
    "        if args.train_file is not None:\n",
    "            data_files['train'] = args.train_file\n",
    "        if args.validation_file is not None:\n",
    "            data_files['validation'] = args.validation_file\n",
    "        extension = args.train_file.split(\".\")[-1]\n",
    "        if extension == 'txt':\n",
    "            extension = 'text'\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "        if \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets = raw_datasets[\"train\"].train_test_split(\n",
    "                test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "            raw_datasets = datasets.DatasetDict(\n",
    "                {\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "logger.info(f\"Loading the model configuration.\")\n",
    "if args.config_name:\n",
    "    config = BertConfig.from_pretrained(args.config_name)\n",
    "elif args.model_name_or_path:\n",
    "    config = BertConfig.from_pretrained(args.model_name_or_path)\n",
    "else:\n",
    "    config = BertConfig(\n",
    "        vocab_size=args.vocab_size,\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "        intermediate_size=args.intermediate_size,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        position_embedding_type=\"absolute\",\n",
    "    )\n",
    "\n",
    "logger.info(f\"Loading the tokenizer.\")\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\n",
    "        args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\n",
    "        args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "logger.info(f\"Initializing Model.\")\n",
    "if args.model_name_or_path:\n",
    "    model = BertForPreTraining.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training a new model from scratch\")\n",
    "    model = BertForPreTraining(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets\n",
    "# First we tokenize all the texts\n",
    "if args.preprocessed == False:\n",
    "    column_names = raw_datasets['train'].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "else:\n",
    "    column_names = [\"text\"]\n",
    "    text_column_name = \"text\"\n",
    "\n",
    "if args.max_seq_length is None:\n",
    "    max_seq_length = tokenizer.model_max_length\n",
    "    if max_seq_length > 1024:\n",
    "        logger.warning(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}).\"\n",
    "            \"Picking 1024 instead. You can change the default value by passing --max_seq_length xxx\"\n",
    "        )\n",
    "        max_seq_length = 1024\n",
    "else:\n",
    "    if args.max_seq_length > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the \"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "\n",
    "def group_texts(examples, idx, split, tokenized_datasets):\n",
    "    # Account for [CLS], [SEP], [SEP]\n",
    "    max_num_tokens = max_seq_length-3\n",
    "    # We *usually* want to fill up the entire sequence since we are padding\n",
    "    # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "    # computation. However, we *sometimes*\n",
    "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "    # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "    # The `target_seq_length` is just a rough target however, whereas\n",
    "    # `max_seq_length` is a hard limit.\n",
    "    target_seq_length = max_num_tokens\n",
    "    if random.random() < args.short_seq_prob:\n",
    "        target_seq_length = random.randint(2, max_num_tokens)\n",
    "    # We DON'T just concatenate all of the tokens from a document into a long\n",
    "    # sequence and choose an arbitrary split point because this would make the\n",
    "    # next sentence prediction task too easy. Instead, we split the input into\n",
    "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "    # input.\n",
    "    result = {k: [] for k, v in tokenizer(\n",
    "        \"\", return_special_tokens_mask=True).items()}\n",
    "    result['next_sentence_label'] = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < len(idx):\n",
    "        segment = {k: examples[k][i][1:-1] for k in examples.keys()}\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment['input_ids'])\n",
    "        if i == len(idx)-1 or current_length >= target_seq_length:\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end = 1\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = random.randint(1, len(current_chunk)-1)\n",
    "                tokens_a = {k: [] for k, t in tokenizer(\n",
    "                    \"\", return_special_tokens_mask=True).items()}\n",
    "                for j in range(a_end):\n",
    "                    for k, v in current_chunk[j].items():\n",
    "                        tokens_a[k].extend(v)\n",
    "\n",
    "                tokens_b = {k: [] for k, t in tokenizer(\n",
    "                    \"\", return_special_tokens_mask=True).items()}\n",
    "                # Random next\n",
    "                is_random_next = False\n",
    "                if len(current_chunk) == 1 or random.random() < args.nsp_probability:\n",
    "                    is_random_next = True\n",
    "                    target_b_length = target_seq_length - \\\n",
    "                        len(tokens_a[\"input_ids\"])\n",
    "                    # This should rarely go for more than one iteration for large\n",
    "                    # corpora. However, just to be careful, we try to make sure that\n",
    "                    # the random document is not the same as the document\n",
    "                    # we're processing.\n",
    "                    for _ in range(10):\n",
    "                        random_segment_index = random.randint(\n",
    "                            0, len(tokenized_datasets[split])-len(idx)-1)\n",
    "                        if (random_segment_index-len(idx) not in idx) and (random_segment_index+len(idx) not in idx):\n",
    "                            break\n",
    "\n",
    "                    random_start = random.randint(0, len(idx)-1)\n",
    "                    for j in range(random_start, len(idx)):\n",
    "                        for k, v in {k: tokenized_datasets[split][random_segment_index+j][k][1:-1] for k in examples.keys()}.items():\n",
    "                            tokens_b[k].extend(v)\n",
    "                        if len(tokens_b['input_ids']) >= target_b_length:\n",
    "                            break\n",
    "                    # We didn't actually use these segments so we \"put them back\" so\n",
    "                    # they don't go to waste.\n",
    "                    num_unused_segments = len(current_chunk)-a_end\n",
    "                    i -= num_unused_segments\n",
    "                # Actual next\n",
    "                else:\n",
    "                    is_random_next = False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        for k, v in current_chunk[j].items():\n",
    "                            tokens_b[k].extend(v)\n",
    "\n",
    "                while True:\n",
    "                    total_length = len(\n",
    "                        tokens_a['input_ids'])+len(tokens_b['input_ids'])\n",
    "                    if total_length <= max_num_tokens:\n",
    "                        break\n",
    "                    trunc_tokens = tokens_a if len(tokens_a['input_ids']) > len(\n",
    "                        tokens_b['input_ids']) else tokens_b\n",
    "                    # We want to sometimes truncate from the front and sometimes from the\n",
    "                    # back to add more randomness and avoid biases.\n",
    "                    if random.random() < 0.5:\n",
    "                        for k in trunc_tokens.keys():\n",
    "                            del trunc_tokens[k][0]\n",
    "                    else:\n",
    "                        for k in trunc_tokens.keys():\n",
    "                            trunc_tokens[k].pop()\n",
    "                inp = {\n",
    "                    k: v[:-1] for k, v in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                for k, v in tokens_a.items():\n",
    "                    inp[k].extend(v)\n",
    "                SEP = {k: v[1:] for k, v in tokenizer(\n",
    "                    \"\", return_special_tokens_mask=True).items()}\n",
    "                for k, v in SEP.items():\n",
    "                    inp[k].extend(v)\n",
    "                tokens_b['token_type_ids'] = list(\n",
    "                    map(lambda x: 1, tokens_b['token_type_ids']))\n",
    "                for k, v in SEP.items():\n",
    "                    tokens_b[k].extend(v)\n",
    "                tokens_b['token_type_ids'][-1] = 1\n",
    "                for k, v in tokens_b.items():\n",
    "                    inp[k].extend(v)\n",
    "                inp['next_sentence_label'] = int(is_random_next)\n",
    "                for k, v in inp.items():\n",
    "                    result[k].append(v)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.preprocessed == False:\n",
    "    logger.info(f\"Beginning Tokenization.\")\n",
    "    if args.line_by_line == True:\n",
    "        # when using line_by_line, we just tokenize each non-empty line.\n",
    "        padding = \"max_length\" if args.pad_to_max_length == True else False\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            # remove empty lines\n",
    "            examples[text_column_name] = [\n",
    "                line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()\n",
    "            ]\n",
    "            return tokenizer(\n",
    "                examples[text_column_name],\n",
    "                padding=padding,\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                # we use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                # receives the `special_tokens_mask`\n",
    "                return_special_tokens_mask=True\n",
    "            )\n",
    "\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=[text_column_name],\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset line by line\",\n",
    "        )\n",
    "        train_dataset = tokenized_datasets[\"train\"]\n",
    "        eval_dataset = tokenized_datasets[\"validation\"]\n",
    "    else:\n",
    "        # otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "        # We use `return_special_tokens=True` because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on every text in dataset\",\n",
    "        )\n",
    "\n",
    "        # Note that with `batched=True`, this map processes 1000 texts together, so group_texts throws away a\n",
    "        # remainder for each of those groups of 1000 texts. You can adjust that batch_size here, but a higher value\n",
    "        # might be slower to preprocess.\n",
    "        #\n",
    "        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "        train_dataset = tokenized_datasets[\"train\"]\n",
    "        eval_dataset = tokenized_datasets[\"validation\"]\n",
    "        logger.info(\n",
    "            f\"Grouping the tokenized dataset into chunks of {max_seq_length}.\")\n",
    "\n",
    "        train_dataset = train_dataset.map(\n",
    "            group_texts,\n",
    "            fn_kwargs={'split': 'train',\n",
    "                       'tokenized_datasets': tokenized_datasets},\n",
    "            batched=True,\n",
    "            batch_size=args.preprocess_batch_size,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            with_indices=True,\n",
    "            desc=f\"Grouping Train texts in chunks of {max_seq_length}\",\n",
    "        )\n",
    "\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            group_texts,\n",
    "            fn_kwargs={'split': 'validation',\n",
    "                       'tokenized_datasets': tokenized_datasets},\n",
    "            batched=True,\n",
    "            batch_size=args.preprocess_batch_size,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            with_indices=True,\n",
    "            desc=f\"Grouping Validation texts in chunks of {max_seq_length}\",\n",
    "        )\n",
    "\n",
    "        prepared_data = datasets.DatasetDict(\n",
    "            {\"train\": train_dataset, \"validation\": eval_dataset})\n",
    "        prepared_data.save_to_disk(\"./data/bookcorpus/bert/prepared\")\n",
    "\n",
    "        dataset = prepared_data['train']\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        def extract_first_sentences(examples):\n",
    "            for i, input_ids in enumerate(examples[\"input_ids\"]):\n",
    "                idx = input_ids.index(tokenizer.sep_token_id)\n",
    "                examples[\"input_ids\"][i] = input_ids[:idx+1]\n",
    "                examples[\"attention_mask\"][i] = examples[\"attention_mask\"][i][:idx+1]\n",
    "                examples[\"token_type_ids\"][i] = examples[\"token_type_ids\"][i][:idx+1]\n",
    "                examples[\"special_tokens_mask\"][i] = examples[\"special_tokens_mask\"][i][:idx+1]\n",
    "            return examples\n",
    "\n",
    "        # filter points from dataset with next_sentence_label == 0\n",
    "\n",
    "        nsp_zero = dataset.filter(lambda examples: [x == 0 for x in examples[\"next_sentence_label\"]],\n",
    "                                  batched=True, num_proc=args.preprocessing_num_workers, keep_in_memory=True)\n",
    "        nsp_zero.save_to_disk(\"./data/bookcorpus/bert/nsp_zero\")\n",
    "\n",
    "        # filter points from dataset with next_sentence_label == 1\n",
    "\n",
    "        nsp_one = dataset.filter(lambda examples: [x == 1 for x in examples[\"next_sentence_label\"]],\n",
    "                                 batched=True, num_proc=args.preprocessing_num_workers, keep_in_memory=True)\n",
    "        nsp_one.save_to_disk(\"./data/bookcorpus/bert/nsp_one\")\n",
    "\n",
    "        # extract first sentences from both datasets\n",
    "\n",
    "        first_sent_nsp_zero = nsp_zero.map(extract_first_sentences, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=[\n",
    "                                           \"next_sentence_label\", \"special_tokens_mask\"], keep_in_memory=True)\n",
    "        first_sent_nsp_zero.save_to_disk(\n",
    "            \"./data/bookcorpus/bert/first_sent_nsp_zero\")\n",
    "\n",
    "        first_sent_nsp_one = nsp_one.map(extract_first_sentences, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=[\n",
    "                                         \"next_sentence_label\", \"special_tokens_mask\"], keep_in_memory=True)\n",
    "        first_sent_nsp_one.save_to_disk(\n",
    "            \"./data/bookcorpus/bert/first_sent_nsp_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data is already preprocessed, load it them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.preprocessed == True:\n",
    "    logger.info(f\"Loading preprocessed dataset(s) from disk.\")\n",
    "    dataset = load_from_disk(\"./data/bookcorpus/bert/prepared\")\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "    nsp_zero = load_from_disk(\"./data/bookcorpus/bert/nsp_one\")\n",
    "    nsp_one = load_from_disk(\"./data/bookcorpus/bert/nsp_zero\")\n",
    "    first_sent_nsp_zero = load_from_disk(\n",
    "        \"./data/bookcorpus/bert/first_sent_nsp_zero\")\n",
    "    first_sent_nsp_one = load_from_disk(\n",
    "        \"./data/bookcorpus/bert/first_sent_nsp_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the random data subset selection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train_dataset' is defined and 'args.subset_fraction' is specified\n",
    "num_samples = int(round(len(train_dataset) * args.subset_fraction, 0))\n",
    "init_subset_indices = random.sample(range(len(train_dataset)), num_samples)\n",
    "\n",
    "# Create subset dataset\n",
    "full_dataset = train_dataset\n",
    "subset_dataset = full_dataset.select(init_subset_indices)\n",
    "\n",
    "# Logging info\n",
    "logger.info(\n",
    "    f\"Full data has {len(full_dataset)} samples, subset data has {len(subset_dataset)} samples.\")\n",
    "if len(train_dataset) > 3:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(\n",
    "            f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collators and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "# This one will take care of the randomly masking the tokens.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n",
    "data_collator_embd = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "logger.info(f\"Creating Data Loaders\")\n",
    "\n",
    "# Create a generator on the CUDA device\n",
    "generator = torch.Generator(device='cuda')\n",
    "# Pass the generator to the RandomSampler\n",
    "sampler = torch.utils.data.RandomSampler(train_dataset, generator=generator)\n",
    "\n",
    "\n",
    "# Dataloaders creation\n",
    "warmstart_dataloader = DataLoader(\n",
    "    train_dataset, sampler=sampler, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "first_sent_nsp_zero_dataloader = DataLoader(\n",
    "    first_sent_nsp_zero, shuffle=False, collate_fn=data_collator_embd, batch_size=args.per_device_eval_batch_size\n",
    ")\n",
    "first_sent_nsp_one_dataloader = DataLoader(\n",
    "    first_sent_nsp_one, shuffle=False, collate_fn=data_collator_embd, batch_size=args.per_device_eval_batch_size\n",
    ")\n",
    "\n",
    "subset_dataloader = DataLoader(\n",
    "    subset_dataset, sampler=sampler, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Intializing optimizer, learning rate schedule\")\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.lr_max_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model for the accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Moving model to GPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if args.selection_strategy in ['fl', 'logdet', 'gc', 'disparity-sum']:\n",
    "    subset_strategy = SubmodStrategy(logger, args.selection_strategy,\n",
    "                                     num_partitions=args.num_partitions, partition_strategy=args.partition_strategy,\n",
    "                                     optimizer=args.optimizer, similarity_criterion='feature',\n",
    "                                     metric='cosine', eta=1, stopIfZeroGain=False,\n",
    "                                     stopIfNegativeGain=False, verbose=False, lambdaVal=1)\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "if hasattr(args.checkpointing_steps, \"isdigit\"):\n",
    "    checkpointing_steps = args.checkpointing_steps\n",
    "    if args.checkpointing_steps.isdigit():\n",
    "        checkpointing_steps = int(args.checkpointing_steps)\n",
    "else:\n",
    "    checkpointing_steps = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warmstarting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * \\\n",
    "    args.gradient_accumulation_steps\n",
    "main_start_time = time.time()\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num warm-start epochs = {args.num_warmstart_epochs}\")\n",
    "logger.info(\n",
    "    f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(\n",
    "    f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps))\n",
    "completed_steps = 0\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint != None:\n",
    "    print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\n",
    "    torch.load(args.resume_from_checkpoint)\n",
    "\n",
    "logger.info(f\"Begin the training.\")\n",
    "timing = []\n",
    "warmstart_start_time = time.time()\n",
    "for epoch in range(args.num_warmstart_epochs):\n",
    "    if epoch == 0:\n",
    "        logger.info(\"Begin the warm-start\")\n",
    "    model.train()\n",
    "    for step, batch in enumerate(warmstart_dataloader):\n",
    "        start_time = time.time()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logger.info(\n",
    "            f\"Completed Steps: {1+completed_steps}; Loss: {loss.detach().float()}; lr: {lr_scheduler.get_last_lr()};\")\n",
    "        loss = loss/args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(warmstart_dataloader)-1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps}\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                # Save model and optimizer state\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    output_dir, 'model_state.pth'))\n",
    "                torch.save(optimizer.state_dict(), os.path.join(\n",
    "                    output_dir, 'optimizer_state.pth'))\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "        timing.append([(time.time() - start_time), 0])\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item() * args.per_device_eval_batch_size)\n",
    "    \n",
    "    \n",
    "    # losses = torch.cat(losses)\n",
    "    losses = losses[:len(eval_dataset)]\n",
    "    losses = torch.tensor(losses).to(device)\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    logger.info(f\"Steps {completed_steps}: perplexity: {perplexity}\")\n",
    "    if epoch == args.num_warmstart_epochs-1:\n",
    "        logger.info(\"End the warm-start\")\n",
    "\n",
    "# Save the final state at the end of warmstart\n",
    "if args.output_dir is not None:\n",
    "    final_output_dir = os.path.join(\n",
    "        args.output_dir, f\"_after_warmstart_step_{completed_steps}\")\n",
    "    os.makedirs(final_output_dir, exist_ok=True)\n",
    "    # Example of saving model and optimizer state\n",
    "    torch.save(model.state_dict(), os.path.join(\n",
    "        final_output_dir, \"model_state.pt\"))\n",
    "    torch.save(optimizer.state_dict(), os.path.join(\n",
    "        final_output_dir, \"optimizer_state.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data subselection & Restructuring for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_nsp_zero = []\n",
    "probs_nsp_one = []\n",
    "greedyList_nsp_zero = []\n",
    "greedyList_nsp_one = []\n",
    "gains_nsp_zero = []\n",
    "gains_nsp_one = []\n",
    "\n",
    "if (args.num_warmstart_epochs != 0) or (args.resume_from_checkpoint != None):\n",
    "    logger.info(\n",
    "        f\"Beginning the subset selection after warm-start or resuming from checkpoint\")\n",
    "    start_time = time.time()\n",
    "    if args.selection_strategy == 'Random-Online':\n",
    "        subset_indices_nsp_zero = [random.sample(\n",
    "            list(range(len(first_sent_nsp_zero))), math.floor(num_samples/2))]\n",
    "        subset_indices_nsp_one = [random.sample(\n",
    "            list(range(len(first_sent_nsp_one))), math.ceil(num_samples/2))]\n",
    "\n",
    "    elif args.selection_strategy in [\"fl\", \"logdet\", \"gc\", \"disparity-min\"]:\n",
    "        logger.info(f\"Performing Subset selection for NSP class 0\")\n",
    "        pbar = tqdm(range(len(first_sent_nsp_zero_dataloader)))\n",
    "        model.eval()\n",
    "        representations_nsp_zero = []\n",
    "        batch_indices_nsp_zero = []\n",
    "        total_cnt = 0\n",
    "        total_storage = 0\n",
    "\n",
    "        representations_start_time = time.time()\n",
    "\n",
    "        for step, batch in enumerate(first_sent_nsp_zero_dataloader):\n",
    "            # Ensure batch data is on the correct device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                output = model(**batch, output_hidden_states=True)\n",
    "\n",
    "            embeddings = output.hidden_states[args.layer_for_similarity_computation]\n",
    "            mask = batch['attention_mask'].unsqueeze(\n",
    "                -1).expand(embeddings.size()).float()\n",
    "            mask1 = (\n",
    "                batch['token_type_ids'].unsqueeze(-1).expand(embeddings.size()).float()) == 0\n",
    "            mask = mask * mask1\n",
    "            mean_pooled = torch.sum(embeddings * mask, 1) / \\\n",
    "                torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "            # Assuming `representations_nsp_zero` is a list to store the embeddings\n",
    "            mean_pooled = mean_pooled.cpu()\n",
    "            total_storage += sys.getsizeof(mean_pooled.storage())\n",
    "            representations_nsp_zero.append(mean_pooled)\n",
    "            pbar.update(1)\n",
    "        # Concatenate tensor representations\n",
    "        representations_nsp_zero = torch.cat(representations_nsp_zero, dim=0)\n",
    "        representations_nsp_zero = representations_nsp_zero[:len(\n",
    "            first_sent_nsp_zero)]\n",
    "        total_storage += sys.getsizeof(representations_nsp_zero.storage())\n",
    "        representations_nsp_zero = representations_nsp_zero.numpy()\n",
    "        logger.info('Representations(NSP Class 0) Size: {}, Total number of samples: {}'.format(\n",
    "            total_storage/(1024 * 1024), total_cnt))\n",
    "\n",
    "        batch_indices_nsp_zero = list(range(len(first_sent_nsp_zero)))\n",
    "        logger.info('Length of indices: {}'.format(\n",
    "            len(batch_indices_nsp_zero)))\n",
    "        logger.info('Representations(NSP Class 0) gathered. Shape of representations: {}. Length of indices: {}'.format(\n",
    "            representations_nsp_zero.shape, len(batch_indices_nsp_zero)))\n",
    "\n",
    "        logger.info(\n",
    "            f\"Representations(NSP Class 0) computed in {time.time() - representations_start_time} seconds\")\n",
    "\n",
    "        # Subset selection strategy\n",
    "        partition_indices_nsp_zero, greedyIdx_nsp_zero, gains_nsp_zero = subset_strategy.select(len(\n",
    "            batch_indices_nsp_zero) - 1, batch_indices_nsp_zero, representations_nsp_zero, parallel_processes=args.parallel_processes, return_gains=True)\n",
    "        subset_indices_nsp_zero = [[]]\n",
    "        i = 0\n",
    "        for p in gains_nsp_zero:\n",
    "            greedyList_nsp_zero.append(greedyIdx_nsp_zero[i:i + len(p)])\n",
    "            i += len(p)\n",
    "\n",
    "        probs_nsp_zero = [taylor_softmax_v1(torch.from_numpy(np.array(\n",
    "            [partition_gains]) / args.temperature)).numpy()[0] for partition_gains in gains_nsp_zero]\n",
    "        rng = np.random.default_rng(args.seed + completed_steps)\n",
    "        for i, partition_prob in enumerate(probs_nsp_zero):\n",
    "            partition_budget = min(math.ceil((len(partition_prob) / len(\n",
    "                batch_indices_nsp_zero)) * math.floor(num_samples / 2)), len(partition_prob) - 1)\n",
    "            subset_indices_nsp_zero[0].extend(rng.choice(\n",
    "                greedyList_nsp_zero[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "\n",
    "        logger.info(f\"Performing Subset selection for NSP class 1\")\n",
    "        pbar = tqdm(range(len(first_sent_nsp_one_dataloader)))\n",
    "        model.eval()\n",
    "        representations_nsp_one = []\n",
    "        batch_indices_nsp_one = []\n",
    "        total_cnt = 0\n",
    "        total_storage = 0\n",
    "        representations_start_time = time.time()\n",
    "        for step, batch in enumerate(first_sent_nsp_one_dataloader):\n",
    "            with torch.no_grad():\n",
    "                # Ensure batch data is on the correct device\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                output = model(**batch, output_hidden_states=True)\n",
    "            embeddings = output.hidden_states[args.layer_for_similarity_computation]\n",
    "            mask = (\n",
    "                batch['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float())\n",
    "            mask1 = (\n",
    "                (batch['token_type_ids'].unsqueeze(-1).expand(embeddings.size()).float()) == 0)\n",
    "            mask = mask * mask1\n",
    "            mean_pooled = torch.sum(embeddings * mask, 1) / \\\n",
    "                torch.clamp(mask.sum(1), min=1e-9)\n",
    "            # Removed accelerator.gather since it's a single-GPU setup\n",
    "            total_cnt += mean_pooled.size(0)\n",
    "            mean_pooled = mean_pooled.cpu()\n",
    "            total_storage += sys.getsizeof(mean_pooled.storage())\n",
    "            representations_nsp_one.append(mean_pooled)\n",
    "            pbar.update(1)\n",
    "\n",
    "        representations_nsp_one = torch.cat(representations_nsp_one, dim=0)\n",
    "        representations_nsp_one = representations_nsp_one[:len(\n",
    "            first_sent_nsp_one)]\n",
    "        total_storage += sys.getsizeof(representations_nsp_one.storage())\n",
    "        representations_nsp_one = representations_nsp_one.numpy()\n",
    "        logger.info('Representations(NSP Class 1) Size: {}, Total number of samples: {}'.format(\n",
    "            total_storage/(1024 * 1024), total_cnt))\n",
    "        batch_indices_nsp_one = list(range(len(first_sent_nsp_one)))\n",
    "        logger.info('Length of indices: {}'.format(len(batch_indices_nsp_one)))\n",
    "        logger.info('Representations(NSP Class 1) gathered. Shape of representations: {}. Length of indices: {}'.format(\n",
    "            representations_nsp_one.shape, len(batch_indices_nsp_one)))\n",
    "        logger.info(\n",
    "            f\"Representations(NSP Class 1) computed in {time.time()-representations_start_time} seconds\")\n",
    "\n",
    "        partition_indices_nsp_one, greedyIdx_nsp_one, gains_nsp_one = subset_strategy.select(len(\n",
    "            batch_indices_nsp_one)-1, batch_indices_nsp_one, representations_nsp_one, parallel_processes=args.parallel_processes, return_gains=True)\n",
    "        subset_indices_nsp_one = [[]]\n",
    "        i = 0\n",
    "        for p in gains_nsp_one:\n",
    "            greedyList_nsp_one.append(greedyIdx_nsp_one[i:i+len(p)])\n",
    "            i += len(p)\n",
    "        probs_nsp_one = [taylor_softmax_v1(torch.from_numpy(np.array(\n",
    "            [partition_gains])/args.temperature)).numpy()[0] for partition_gains in gains_nsp_one]\n",
    "        rng = np.random.default_rng(args.seed+completed_steps)\n",
    "        for i, partition_prob in enumerate(probs_nsp_one):\n",
    "            partition_budget = min(math.ceil(\n",
    "                (len(partition_prob)/len(batch_indices_nsp_one)) * math.ceil(num_samples/2)), len(partition_prob)-1)\n",
    "            subset_indices_nsp_one[0].extend(rng.choice(\n",
    "                greedyList_nsp_one[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "\n",
    "\n",
    "output_file = f\"nsp_zero_subset_indices_after_step_{completed_steps}.pt\"\n",
    "output_file = os.path.join(args.subset_dir, output_file)\n",
    "torch.save(torch.tensor(subset_indices_nsp_zero), output_file)\n",
    "output_file = f\"nsp_one_subset_indices_after_step_{completed_steps}.pt\"\n",
    "output_file = os.path.join(args.subset_dir, output_file)\n",
    "torch.save(torch.tensor(subset_indices_nsp_one), output_file)\n",
    "output_file = f\"nsp_zero_gains_after_step_{completed_steps}.pkl\"\n",
    "output_file = os.path.join(args.subset_dir, output_file)\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(gains_nsp_zero, f)\n",
    "output_file = f\"nsp_one_gains_after_step_{completed_steps}.pkl\"\n",
    "output_file = os.path.join(args.subset_dir, output_file)\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(gains_nsp_one, f)\n",
    "output_file = f\"nsp_zero_partition_indices_after_step_{completed_steps}.pkl\"\n",
    "output_file = os.path.join(args.subset_dir, output_file)\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(partition_indices_nsp_zero, f)\n",
    "output_file = f\"nsp_one_partition_indices_after_step_{completed_steps}.pkl\"\n",
    "output_file = os.path.join(args.subset_dir, output_file)\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(partition_indices_nsp_one, f)\n",
    "output_file = f\"nsp_zero_greedy_indices_after_step_{completed_steps}.pkl\"\n",
    "output_file = os.path.join(args.subset_dir, output_file)\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(greedyIdx_nsp_zero, f)\n",
    "output_file = f\"nsp_one_greedy_indices_after_step_{completed_steps}.pkl\"\n",
    "output_file = os.path.join(args.subset_dir, output_file)\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(greedyIdx_nsp_one, f)\n",
    "\n",
    "nsp_zero_subset_dataset = nsp_zero.select(subset_indices_nsp_zero[0])\n",
    "nsp_one_subset_dataset = nsp_one.select(subset_indices_nsp_one[0])\n",
    "# Concatenate the two datasets\n",
    "subset_dataset = concatenate_datasets(\n",
    "    [nsp_zero_subset_dataset, nsp_one_subset_dataset])\n",
    "subset_dataloader = DataLoader(\n",
    "    subset_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    \"Begin the main training loop with importance re-sampling, after warm-start\")\n",
    "while completed_steps < args.max_train_steps:\n",
    "    model.train()\n",
    "    select_subset = False\n",
    "    for step, batch in enumerate(subset_dataloader):\n",
    "        train_time = 0\n",
    "        subset_time = 0\n",
    "        start_time = time.time()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logger.info(\n",
    "            f\"Completed Steps: {1+completed_steps}; Loss: {loss.detach().float()}; lr: {lr_scheduler.get_last_lr()};\")\n",
    "        loss = loss/args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(subset_dataloader)-1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "        train_time += (time.time() - start_time)\n",
    "\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps}\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                # Save model and optimizer state\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    output_dir, 'model_state.pth'))\n",
    "                torch.save(optimizer.state_dict(), os.path.join(\n",
    "                    output_dir, 'optimizer_state.pth'))\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "        if (completed_steps) % args.select_every == 0:\n",
    "            select_subset = True\n",
    "            break\n",
    "        timing.append([train_time, subset_time])\n",
    "    if select_subset == True:\n",
    "        start_time = time.time()\n",
    "        num_samples = int(round(len(full_dataset) * args.subset_fraction, 0))\n",
    "        if args.selection_strategy == 'Random-Online':\n",
    "            subset_indices_nsp_zero = [random.sample(\n",
    "                list(range(len(first_sent_nsp_zero))), math.floor(num_samples/2))]\n",
    "            subset_indices_nsp_one = [random.sample(\n",
    "                list(range(len(first_sent_nsp_one))), math.ceil(num_samples/2))]\n",
    "\n",
    "        elif args.selection_strategy in [\"fl\", \"logdet\", \"gc\", \"disparity-min\"]:\n",
    "            logger.info(f\"Performing Subset selection for NSP class 0\")\n",
    "            sampling_start_time = time.time()\n",
    "            subset_indices_nsp_zero = [[]]\n",
    "            rng = np.random.default_rng(args.seed+completed_steps)\n",
    "            for i, partition_prob in enumerate(probs_nsp_zero):\n",
    "                partition_budget = min(math.ceil((len(partition_prob)/len(\n",
    "                    batch_indices_nsp_zero)) * math.floor(num_samples/2)), len(partition_prob)-1)\n",
    "                subset_indices_nsp_zero[0].extend(rng.choice(\n",
    "                    greedyList_nsp_zero[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "\n",
    "            logger.info(f\"Performing Subset selection for NSP class 1\")\n",
    "            sampling_start_time = time.time()\n",
    "            subset_indices_nsp_one = [[]]\n",
    "            rng = np.random.default_rng(args.seed+completed_steps)\n",
    "            for i, partition_prob in enumerate(probs_nsp_one):\n",
    "                partition_budget = min(math.ceil(\n",
    "                    (len(partition_prob)/len(batch_indices_nsp_one)) * math.ceil(num_samples/2)), len(partition_prob)-1)\n",
    "                subset_indices_nsp_one[0].extend(rng.choice(\n",
    "                    greedyList_nsp_one[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "\n",
    "            logger.info(\"Sampling time(NSP Class 1): {}\".format(\n",
    "                time.time()-sampling_start_time))\n",
    "\n",
    "        timing.append([0, time.time()-start_time])\n",
    "\n",
    "        output_file = f\"nsp_zero_subset_indices_after_step_{completed_steps}.pt\"\n",
    "        output_file = os.path.join(args.subset_dir, output_file)\n",
    "        torch.save(torch.tensor(subset_indices_nsp_zero), output_file)\n",
    "        output_file = f\"nsp_one_subset_indices_after_step_{completed_steps}.pt\"\n",
    "        output_file = os.path.join(args.subset_dir, output_file)\n",
    "        torch.save(torch.tensor(subset_indices_nsp_one), output_file)\n",
    "\n",
    "        nsp_zero_subset_dataset = nsp_zero.select(subset_indices_nsp_zero[0])\n",
    "        nsp_one_subset_dataset = nsp_one.select(subset_indices_nsp_one[0])\n",
    "\n",
    "        # Concatenate the two datasets\n",
    "        subset_dataset = concatenate_datasets(\n",
    "            [nsp_zero_subset_dataset, nsp_one_subset_dataset])\n",
    "        subset_dataloader = DataLoader(\n",
    "            subset_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "        subset_time = time.time()-start_time\n",
    "        select_subset = False\n",
    "        timing.append([0, subset_time])\n",
    "        logger.info(\n",
    "            f\"Subset selection time(total resampling time): {time.time()-start_time} seconds.\")\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item() * args.per_device_eval_batch_size)\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[:len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    logger.info(f\"Steps {completed_steps}: perplexity: {perplexity}\")\n",
    "\n",
    "logger.info(f\"Timing: {timing}\")\n",
    "logger.info(f\"Saving the final model after {completed_steps} steps.\")\n",
    "if args.output_dir is not None:\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the model using the Transformers library's built-in save_pretrained method\n",
    "    model.save_pretrained(args.output_dir)\n",
    "\n",
    "    # Save the tokenizer\n",
    "    if hasattr(tokenizer, 'save_pretrained'):\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "logger.info(\n",
    "    f\"Training completed successfully in {time.time() - main_start_time} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
