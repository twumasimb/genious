{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twumasimb/miniconda3/envs/ingenious/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import math\n",
    "import random\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizerFast,\n",
    "    BertForPreTraining,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    "    SchedulerType)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from selectionstrategies import SubmodStrategy\n",
    "from helper_fns import taylor_softmax_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "raw_dataset = load_from_disk('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "# Split dataset\n",
    "wiki_dataset=wiki_dataset[\"train\"].train_test_split(test_size=(30/100), shuffle=False)\n",
    "wiki_dataset=datasets.DatasetDict({\"train\": wiki_dataset[\"train\"], \"validation\": wiki_dataset[\"test\"]})\n",
    "# saving the dataset\n",
    "wiki_dataset.save_to_disk(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "print(\"Creating Tokenizer\")\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model \n",
    "config = BertConfig()\n",
    "model = BertForPreTraining(config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names for tokenization\n",
    "column_names = wiki_dataset[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "# set length for tokenization\n",
    "max_seq_length = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    examples[text_column_name] = [\n",
    "        line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    return tokenizer(examples[text_column_name], truncation=True, max_length=max_seq_length, padding=\"max_length\", return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "print(\"Tokenizing the dataset\")\n",
    "tokenized_wiki_dataset = wiki_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=4, \n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on the entire dataset\",\n",
    ")\n",
    "train_dataset = tokenized_wiki_dataset[\"train\"]\n",
    "validation_dataset = tokenized_wiki_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_wiki_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples, idx, split, tokenized_datasets):\n",
    "    # Account for [CLS], [SEP], [SEP]\n",
    "    max_num_tokens = max_seq_length-3\n",
    "    # We *usually* want to fill up the entire sequence since we are padding\n",
    "    # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "    # computation. However, we *sometimes*\n",
    "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "    # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "    # The `target_seq_length` is just a rough target however, whereas\n",
    "    # `max_seq_length` is a hard limit.\n",
    "    short_seq_prob = 0.1\n",
    "    nsp_probability = 0.5\n",
    "    target_seq_length = max_num_tokens\n",
    "    if random.random() < short_seq_prob:\n",
    "        target_seq_length = random.randint(2, max_num_tokens)\n",
    "    # We DON'T just concatenate all of the tokens from a document into a long\n",
    "    # sequence and choose an arbitrary split point because this would make the\n",
    "    # next sentence prediction task too easy. Instead, we split the input into\n",
    "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "    # input.\n",
    "    result = {k: [] for k, v in tokenizer(\n",
    "        \"\", return_special_tokens_mask=True).items()}\n",
    "    result['next_sentence_label'] = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < len(idx):\n",
    "        segment = {k: examples[k][i][1:-1] for k in examples.keys()}\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment['input_ids'])\n",
    "        if i == len(idx)-1 or current_length >= target_seq_length:\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end = 1\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = random.randint(1, len(current_chunk)-1)\n",
    "                tokens_a = {k: [] for k, t in tokenizer(\n",
    "                    \"\", return_special_tokens_mask=True).items()}\n",
    "                for j in range(a_end):\n",
    "                    for k, v in current_chunk[j].items():\n",
    "                        tokens_a[k].extend(v)\n",
    "\n",
    "                tokens_b = {k: [] for k, t in tokenizer(\n",
    "                    \"\", return_special_tokens_mask=True).items()}\n",
    "                # Random next\n",
    "                is_random_next = False\n",
    "                if len(current_chunk) == 1 or random.random() < nsp_probability:\n",
    "                    is_random_next = True\n",
    "                    target_b_length = target_seq_length - \\\n",
    "                        len(tokens_a[\"input_ids\"])\n",
    "                    # This should rarely go for more than one iteration for large\n",
    "                    # corpora. However, just to be careful, we try to make sure that\n",
    "                    # the random document is not the same as the document\n",
    "                    # we're processing.\n",
    "                    for _ in range(10):\n",
    "                        random_segment_index = random.randint(\n",
    "                            0, len(tokenized_datasets[split])-len(idx)-1)\n",
    "                        if (random_segment_index-len(idx) not in idx) and (random_segment_index+len(idx) not in idx):\n",
    "                            break\n",
    "\n",
    "                    random_start = random.randint(0, len(idx)-1)\n",
    "                    for j in range(random_start, len(idx)):\n",
    "                        for k, v in {k: tokenized_datasets[split][random_segment_index+j][k][1:-1] for k in examples.keys()}.items():\n",
    "                            tokens_b[k].extend(v)\n",
    "                        if len(tokens_b['input_ids']) >= target_b_length:\n",
    "                            break\n",
    "                    # We didn't actually use these segments so we \"put them back\" so\n",
    "                    # they don't go to waste.\n",
    "                    num_unused_segments = len(current_chunk)-a_end\n",
    "                    i -= num_unused_segments\n",
    "                # Actual next\n",
    "                else:\n",
    "                    is_random_next = False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        for k, v in current_chunk[j].items():\n",
    "                            tokens_b[k].extend(v)\n",
    "\n",
    "                while True:\n",
    "                    total_length = len(\n",
    "                        tokens_a['input_ids'])+len(tokens_b['input_ids'])\n",
    "                    if total_length <= max_num_tokens:\n",
    "                        break\n",
    "                    trunc_tokens = tokens_a if len(tokens_a['input_ids']) > len(\n",
    "                        tokens_b['input_ids']) else tokens_b\n",
    "                    # We want to sometimes truncate from the front and sometimes from the\n",
    "                    # back to add more randomness and avoid biases.\n",
    "                    if random.random() < 0.5:\n",
    "                        for k in trunc_tokens.keys():\n",
    "                            del trunc_tokens[k][0]\n",
    "                    else:\n",
    "                        for k in trunc_tokens.keys():\n",
    "                            trunc_tokens[k].pop()\n",
    "                inp = {\n",
    "                    k: v[:-1] for k, v in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                for k, v in tokens_a.items():\n",
    "                    inp[k].extend(v)\n",
    "                SEP = {k: v[1:] for k, v in tokenizer(\n",
    "                    \"\", return_special_tokens_mask=True).items()}\n",
    "                for k, v in SEP.items():\n",
    "                    inp[k].extend(v)\n",
    "                tokens_b['token_type_ids'] = list(\n",
    "                    map(lambda x: 1, tokens_b['token_type_ids']))\n",
    "                for k, v in SEP.items():\n",
    "                    tokens_b[k].extend(v)\n",
    "                tokens_b['token_type_ids'][-1] = 1\n",
    "                for k, v in tokens_b.items():\n",
    "                    inp[k].extend(v)\n",
    "                inp['next_sentence_label'] = int(is_random_next)\n",
    "                for k, v in inp.items():\n",
    "                    result[k].append(v)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    group_texts,\n",
    "    fn_kwargs={'split': 'train', 'tokenized_datasets': tokenized_wiki_dataset},\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=8,\n",
    "    load_from_cache_file=False,\n",
    "    with_indices=True,\n",
    "    desc=f\"Grouping Train texts in chunks of {max_seq_length}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the validation dataset\n",
    "validation_dataset = validation_dataset.map(\n",
    "    group_texts,\n",
    "    fn_kwargs={'split': 'validation', 'tokenized_datasets': tokenized_wiki_dataset},\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=8,\n",
    "    load_from_cache_file= False,\n",
    "    with_indices=True,\n",
    "    desc=f\"Grouping Validation texts in chunks of {max_seq_length}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Subset Selection\n",
    "subset_fraction = 0.1\n",
    "num_samples = int(round(len(train_dataset) * subset_fraction, 0))\n",
    "init_subset_indices = [random.sample(list(range(len(train_dataset))), num_samples)]\n",
    "\n",
    "full_dataset=train_dataset\n",
    "subset_dataset = full_dataset.select(init_subset_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_probability=0.15\n",
    "per_device_train_batch_size=32\n",
    "per_device_eval_batch_size=32\n",
    "\n",
    "data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "\n",
    "# Dataloaders creation\n",
    "warmstart_dataloader=DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=per_device_train_batch_size\n",
    ")\n",
    "\n",
    "subset_dataloader=DataLoader(\n",
    "    subset_dataset, shuffle=True, collate_fn=data_collator, batch_size=per_device_train_batch_size\n",
    ")\n",
    "\n",
    "eval_dataloader=DataLoader(\n",
    "    validation_dataset, collate_fn=data_collator, batch_size=per_device_eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Optimizer & Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not\n",
    "\n",
    "weight_decay=0.01\n",
    "learning_rate=5e-5\n",
    "\n",
    "no_decay=[\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters=[\n",
    "    {\n",
    "        \"params\":[p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\":weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\":[p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer=AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler_type=SchedulerType.LINEAR\n",
    "num_warmup_steps=1000\n",
    "num_training_steps=10000\n",
    "\n",
    "lr_scheduler=get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 1500\n",
    "partition_strategy = 'random'\n",
    "ss_optimizer = 'LazyGreedy'\n",
    "subset_strategy = SubmodStrategy(logger=None, smi_func_type='fl',\n",
    "                                 num_partitions=num_partitions, partition_strategy=partition_strategy,\n",
    "                                 optimizer=ss_optimizer, similarity_criterion='feature',\n",
    "                                 metric='cosine', eta=1, stopIfZeroGain=False,\n",
    "                                 stopIfNegativeGain=False, verbose=False, lambdaVal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_train_steps = 10000\n",
    "per_device_train_batch_size = 32\n",
    "num_warmstart_epochs = 100\n",
    "num_processes = 1\n",
    "gradient_accumulation_steps = 1\n",
    "checkpointing_steps = 1000\n",
    "output_dir = \"\"\n",
    "\n",
    "# Train!\n",
    "total_batch_size = per_device_train_batch_size * num_processes * gradient_accumulation_steps\n",
    "main_start_time = time.time()\n",
    "print(f\"  Num examples = {len(train_dataset)}\")\n",
    "print(f\"  Num warm-start epochs = {num_warmstart_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {per_device_train_batch_size}\")\n",
    "print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "print(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(max_train_steps))\n",
    "completed_steps = 0\n",
    "\n",
    "print(f\"Begin the training.\")\n",
    "timing = []\n",
    "warmstart_start_time = time.time()\n",
    "for epoch in range(num_warmstart_epochs):\n",
    "    if epoch == 0:\n",
    "        print(\"Begin the warm-start\")\n",
    "    model.train()\n",
    "    for step, batch in enumerate(warmstart_dataloader):\n",
    "        start_time = time.time()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        print(f\"Completed Steps: {1+completed_steps}; Loss: {loss.detach().float()}; lr: {lr_scheduler.get_last_lr()};\")\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if step % gradient_accumulation_steps == 0 or step == len(warmstart_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps}\"\n",
    "                if output_dir is not None:\n",
    "                    output_dir = os.path.join(output_dir, output_dir)\n",
    "                torch.save(model.state_dict(), output_dir)\n",
    "        if completed_steps >= max_train_steps:\n",
    "            break\n",
    "        timing.append([(time.time() - start_time), 0])\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(per_device_eval_batch_size))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[:len(validation_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\"Steps {completed_steps}: perplexity: {perplexity}\")\n",
    "    if epoch == num_warmstart_epochs - 1:\n",
    "        print(\"End the warm-start\")\n",
    "# Save the state after warm-start\n",
    "output_dir = f\"after_warmstart_step_{completed_steps}\"\n",
    "if output_dir is not None:\n",
    "    output_dir = os.path.join(output_dir, output_dir)\n",
    "torch.save(model.state_dict(), output_dir)\n",
    "warmstart_end_time = time.time()\n",
    "print(f\"Completed warm-start in {warmstart_end_time - warmstart_start_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingenious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
