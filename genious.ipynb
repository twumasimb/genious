{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datasets\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed, broadcast_object_list\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizerFast,\n",
    "    BertForPreTraining,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")\n",
    "from transformers.utils.versions import require_version\n",
    "from selectionstrategies import SubmodStrategy\n",
    "from accelerate import InitProcessGroupKwargs\n",
    "from helper_fns import taylor_softmax_v1\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Initial parameters and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger=get_logger(__name__)\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r requirements.txt\")\n",
    "\n",
    "def parse_args():\n",
    "    parser=argparse.ArgumentParser(description=\"Train a BERT on Masked Language Modeling and Next Sentence Prediction tasks using the INGENIOUS subset selection strategy\")\n",
    "    parser.add_argument(\n",
    "        \"--log_dir\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The directory to which training logs should be written\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--subset_dir\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The directory to which information regarding selected subsets should be stored\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--preprocessed\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, already preprocessed data needs to be given and training will start right away without preprocessing\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_data_from_disk\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, the dataset is loaded from the disk instead of downloading from the hub\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_directory\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The path to the directory containing the dataset in case of load_data_from_disk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The name of the dataset to use (via the datasets library).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset_config_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The configuration name of the dataset to use (via the datasets library).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_file\", type=str, default=None, help=\"A csv or a json file containing the training data.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_file\", type=str, default=None, help=\"A csv or a json file containing the validation data.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_split_percentage\",\n",
    "        type=float,\n",
    "        default=5,\n",
    "        help=\"The percentage of the train set used as validation set in case there's no validation split\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pad_to_max_length\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Pretrained config name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_size\", type=int, default=768, help=\"Hidden Size of language model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_hidden_layers\", type=int, default=12, help=\"Num Hidden Layers\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_attention_heads\", type=int, default=12, help=\"Num attention heads\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intermediate_size\", type=int, default=3072, help=\"Intermediate size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--vocab_size\",\n",
    "        type=int,\n",
    "        default=30522,\n",
    "        help=\"The size of vocabulary used by the tokenizer\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_slow_tokenizer\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, will use a slow tokenizer (not backed by the ðŸ¤— Tokenizers library).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_train_batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size (per device) for the training dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_eval_batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size (per device) for the evaluation dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_max_steps\",\n",
    "        type=int,\n",
    "        default=1000000,\n",
    "        help=\"Max training steps for learning rate. (Can tune the rate of decay of the learning rate with this parameter)\"\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=250000,\n",
    "        help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler_type\",\n",
    "        type=SchedulerType,\n",
    "        default=\"linear\",\n",
    "        help=\"The scheduler type to use.\",\n",
    "        choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_warmup_steps\", type=int, default=10000, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
    "    )\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=None, help=\"Where to store the final model.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n",
    "    parser.add_argument(\n",
    "        \"--max_seq_length\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--line_by_line\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--preprocessing_num_workers\",\n",
    "        type=int,\n",
    "        default=96,\n",
    "        help=\"The number of processes to use for the preprocessing.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--preprocess_batch_size\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"batch size during preprocessing\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_cache\", type=bool, default=False, help=\"Overwrite the cached training and evaluation sets\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mlm_probability\", type=float, default=0.15, help=\"Ratio of tokens to mask for masked language modeling loss\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--short_seq_prob\", type=float, default=0.1, help=\"Fraction of input sentences which are not of maximum token length possible\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nsp_probability\", type=float, default=0.5, help=\"Fraction of incorrect sentence pairs in all of the input\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--subset_fraction\", type=float, default=0.25, help=\"Fraction of the dataset that we want to use for training\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--selection_strategy\", type=str, default='fl', help=\"Subset selection strategy\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--optimizer\", type=str, default=\"LazyGreedy\", help=\"Optimizer to use for subset selection\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--select_every\", type=int, default=25000, help=\"Select a new subset for training every select_every training steps\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--partition_strategy\", type=str, default=\"random\", help=\"Partition strategy for subset selection\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--layer_for_similarity_computation\", type=int, default=9, help=\"The hidden layer to use while calculating the similarities in submodular functions\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_partitions\", type=int, default=5000, help=\"Number of partitions in subset selection\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--parallel_processes\", type=int, default=96, help=\"Number of parallel processes for subset selection\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_warmstart_epochs\", type=int, default=0, help=\"Number of epochs to run in the warmstart stage\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Whether various states should be saved at the end of every n steps\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"If the training should continue from a checkpoint folder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--temperature\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"temperature while computing the Taylor Softmax\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Parameters for args in argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to work on the argparse to pass the values of the arguments to the main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Logger and Argsparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=parse_args()\n",
    "init_process_group=InitProcessGroupKwargs(timeout=datetime.timedelta(seconds=75000))\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "accelerator=Accelerator(kwargs_handlers=[init_process_group])\n",
    "# Make one log on every process with the configuration for debugging\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(args.log_dir,\"train_logs.log\"),\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "# logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "if not args.preprocessed:\n",
    "    logger.info(f\"Data is not preprocessed.\")\n",
    "    logger.info(f\"Loading the data.\")\n",
    "    if args.load_data_from_disk is not None:\n",
    "        if args.data_directory is not None:\n",
    "            raw_datasets=load_from_disk(args.data_directory)\n",
    "            if \"validation\" not in raw_datasets.keys():\n",
    "                raw_datasets=raw_datasets[\"train\"].train_test_split(test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "                raw_datasets=datasets.DatasetDict({\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})\n",
    "    elif args.dataset_name is not None:\n",
    "        raw_datasets=load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "        if \"validaton\" not in raw_datasets.keys():\n",
    "            raw_datasets=raw_datasets[\"train\"].train_test_split(test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "            raw_datasets=datasets.DatasetDict({\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})\n",
    "    else:\n",
    "        data_files={}\n",
    "        if args.train_file is not None:\n",
    "            data_files['train']=args.train_file\n",
    "        if args.validation_file is not None:\n",
    "            data_files['validation']=args.validation_file\n",
    "        extension=args.train_file.split(\".\")[-1]\n",
    "        if extension=='txt':\n",
    "            extension='text'\n",
    "        raw_datasets=load_dataset(extension, data_files=data_files)\n",
    "        if \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets=raw_datasets[\"train\"].train_test_split(test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "            raw_datasets=datasets.DatasetDict({\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "logger.info(f\"Loading the model configuration.\")\n",
    "if args.config_name:\n",
    "    config=BertConfig.from_pretrained(args.config_name)\n",
    "elif args.model_name_or_path:\n",
    "    config=BertConfig.from_pretrained(args.model_name_or_path)\n",
    "else:\n",
    "    config=BertConfig(\n",
    "        vocab_size=args.vocab_size,\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "        intermediate_size=args.intermediate_size,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        position_embedding_type=\"absolute\",\n",
    "    )\n",
    "\n",
    "logger.info(f\"Loading the tokenizer.\")\n",
    "if args.tokenizer_name:\n",
    "    tokenizer=BertTokenizerFast.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer=BertTokenizerFast.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "logger.info(f\"Initializing Model.\")\n",
    "if args.model_name_or_path:\n",
    "    model=BertForPreTraining.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\"in args.model_name_or_path),\n",
    "        config=config\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training a new model from scratch\")\n",
    "    model=BertForPreTraining(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the datasets\n",
    "#First we tokenize all the texts\n",
    "if not args.preprocessed:\n",
    "    column_names=raw_datasets['train'].column_names\n",
    "    text_column_name=\"text\" if \"text\" in column_names else column_names[0]\n",
    "else:\n",
    "    column_names=[\"text\"]\n",
    "    text_column_name=\"text\"\n",
    "\n",
    "if args.max_seq_length is None:\n",
    "    max_seq_length=tokenizer.model_max_length\n",
    "    if max_seq_length>1024:\n",
    "        logger.warning(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}).\"\n",
    "            \"Picking 1024 instead. You can change the default value by passing --max_seq_length xxx\"\n",
    "        )\n",
    "        max_seq_length=1024\n",
    "else:\n",
    "    if args.max_seq_length>tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the \"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length=min(args.max_seq_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main data processing function that will concatenate all texts from our dataset and generate chunks of \n",
    "#max_seq_length.\n",
    "\n",
    "def group_texts(examples, idx, split, tokenized_datasets):\n",
    "    # Account for [CLS], [SEP], [SEP]\n",
    "    max_num_tokens=max_seq_length-3\n",
    "    # We *usually* want to fill up the entire sequence since we are padding\n",
    "    # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "    # computation. However, we *sometimes*\n",
    "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "    # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "    # The `target_seq_length` is just a rough target however, whereas\n",
    "    # `max_seq_length` is a hard limit.\n",
    "    target_seq_length=max_num_tokens\n",
    "    if random.random()<args.short_seq_prob:\n",
    "        target_seq_length=random.randint(2, max_num_tokens)\n",
    "    # We DON'T just concatenate all of the tokens from a document into a long\n",
    "    # sequence and choose an arbitrary split point because this would make the\n",
    "    # next sentence prediction task too easy. Instead, we split the input into\n",
    "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "    # input.\n",
    "    result={k: [] for k, v in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "    result['next_sentence_label']=[]\n",
    "    current_chunk=[]\n",
    "    current_length=0\n",
    "    i=0 \n",
    "    while i<len(idx):\n",
    "        segment={k: examples[k][i][1:-1] for k in examples.keys()}\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment['input_ids'])\n",
    "        if i==len(idx)-1 or current_length>=target_seq_length:\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end=1\n",
    "                if len(current_chunk)>=2:\n",
    "                    a_end=random.randint(1, len(current_chunk)-1)\n",
    "                tokens_a={k: [] for k, t in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                for j in range(a_end):\n",
    "                    for k, v in current_chunk[j].items():\n",
    "                        tokens_a[k].extend(v)\n",
    "\n",
    "                tokens_b={k: [] for k, t in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                # Random next\n",
    "                is_random_next=False\n",
    "                if len(current_chunk)==1 or random.random()<args.nsp_probability:\n",
    "                    is_random_next=True\n",
    "                    target_b_length=target_seq_length-len(tokens_a[\"input_ids\"])\n",
    "                    # This should rarely go for more than one iteration for large\n",
    "                    # corpora. However, just to be careful, we try to make sure that\n",
    "                    # the random document is not the same as the document\n",
    "                    # we're processing.\n",
    "                    for _ in range(10):\n",
    "                        random_segment_index=random.randint(0, len(tokenized_datasets[split])-len(idx)-1)\n",
    "                        if (random_segment_index-len(idx) not in idx) and (random_segment_index+len(idx) not in idx):\n",
    "                            break\n",
    "\n",
    "                    random_start=random.randint(0, len(idx)-1)\n",
    "                    for j in range(random_start, len(idx)):\n",
    "                        for k, v in {k: tokenized_datasets[split][random_segment_index+j][k][1:-1] for k in examples.keys()}.items():\n",
    "                            tokens_b[k].extend(v)\n",
    "                        if len(tokens_b['input_ids'])>=target_b_length:\n",
    "                            break\n",
    "                    # We didn't actually use these segments so we \"put them back\" so\n",
    "                    # they don't go to waste.\n",
    "                    num_unused_segments=len(current_chunk)-a_end\n",
    "                    i-=num_unused_segments\n",
    "                # Actual next\n",
    "                else:\n",
    "                    is_random_next=False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        for k, v in current_chunk[j].items():\n",
    "                            tokens_b[k].extend(v)\n",
    "\n",
    "                while True:\n",
    "                    total_length=len(tokens_a['input_ids'])+len(tokens_b['input_ids'])\n",
    "                    if total_length<=max_num_tokens:\n",
    "                        break\n",
    "                    trunc_tokens= tokens_a if len(tokens_a['input_ids'])>len(tokens_b['input_ids']) else tokens_b\n",
    "                    # We want to sometimes truncate from the front and sometimes from the\n",
    "                    # back to add more randomness and avoid biases.\n",
    "                    if random.random()<0.5:\n",
    "                        for k in trunc_tokens.keys():\n",
    "                            del trunc_tokens[k][0]\n",
    "                    else:\n",
    "                        for k in trunc_tokens.keys():\n",
    "                            trunc_tokens[k].pop()\n",
    "                inp={k: v[:-1] for k, v in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                for k, v in tokens_a.items():\n",
    "                    inp[k].extend(v)\n",
    "                SEP={k: v[1:] for k, v in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                for k, v in SEP.items():\n",
    "                    inp[k].extend(v)\n",
    "                tokens_b['token_type_ids']=list(map(lambda x: 1, tokens_b['token_type_ids']))\n",
    "                for k, v in SEP.items():\n",
    "                    tokens_b[k].extend(v)\n",
    "                tokens_b['token_type_ids'][-1]=1\n",
    "                for k, v in tokens_b.items():\n",
    "                    inp[k].extend(v)\n",
    "                inp['next_sentence_label']=int(is_random_next)\n",
    "                for k, v in inp.items():\n",
    "                    result[k].append(v)\n",
    "            current_chunk=[]\n",
    "            current_length=0\n",
    "        i+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.preprocessed:\n",
    "    logger.info(f\"Beginning Tokenization.\")\n",
    "    if args.line_by_line:\n",
    "        #when using line_by_line, we just tokenize each non-empty line.\n",
    "        padding=\"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            #remove empty lines\n",
    "            examples[text_column_name]=[\n",
    "                line for line in examples[text_column_name] if len(line)>0 and not line.isspace()\n",
    "            ]\n",
    "            return tokenizer(\n",
    "                examples[text_column_name],\n",
    "                padding=padding,\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                #we use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                #receives the `special_tokens_mask`\n",
    "                return_special_tokens_mask=True\n",
    "            )\n",
    "        \n",
    "        with accelerator.main_process_first():\n",
    "            tokenized_datasets=raw_datasets.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=args.preprocessing_num_workers,\n",
    "                remove_columns=[text_column_name],\n",
    "                load_from_cache_file=not args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on dataset line by line\",\n",
    "            )\n",
    "        train_dataset=tokenized_datasets[\"train\"]\n",
    "        eval_dataset=tokenized_datasets[\"validation\"]\n",
    "    else:\n",
    "        # otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "        # We use `return_special_tokens=True` because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "        with accelerator.main_process_first():\n",
    "            tokenized_datasets=raw_datasets.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=args.preprocessing_num_workers,\n",
    "                remove_columns=column_names,\n",
    "                load_from_cache_file=not args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on every text in dataset\",\n",
    "            )\n",
    "        \n",
    "        # Note that with `batched=True`, this map processes 1000 texts together, so group_texts throws away a \n",
    "        # remainder for each of those groups of 1000 texts. You can adjust that batch_size here, but a higher value\n",
    "        # might be slower to preprocess.\n",
    "        #\n",
    "        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "        train_dataset=tokenized_datasets[\"train\"]\n",
    "        eval_dataset=tokenized_datasets[\"validation\"]\n",
    "        logger.info(f\"Grouping the tokenized dataset into chunks of {max_seq_length}.\")\n",
    "        with accelerator.main_process_first():\n",
    "            train_dataset=train_dataset.map(\n",
    "                group_texts,\n",
    "                fn_kwargs={'split': 'train', 'tokenized_datasets': tokenized_datasets},\n",
    "                batched=True,\n",
    "                batch_size=args.preprocess_batch_size,\n",
    "                num_proc=args.preprocessing_num_workers,\n",
    "                load_from_cache_file=not args.overwrite_cache,\n",
    "                with_indices=True,\n",
    "                desc=f\"Grouping Train texts in chunks of {max_seq_length}\",\n",
    "            )\n",
    "        with accelerator.main_process_first():\n",
    "            eval_dataset=eval_dataset.map(\n",
    "                group_texts,\n",
    "                fn_kwargs={'split': 'validation', 'tokenized_datasets': tokenized_datasets},\n",
    "                batched=True,\n",
    "                batch_size=args.preprocess_batch_size,\n",
    "                num_proc=args.preprocessing_num_workers,\n",
    "                load_from_cache_file=not args.overwrite_cache,\n",
    "                with_indices=True,\n",
    "                desc=f\"Grouping Validation texts in chunks of {max_seq_length}\",\n",
    "            )\n",
    "\n",
    "        prepared_data  = datasets.DatasetDict({\"train\": train_dataset, \"validation\": eval_dataset})\n",
    "        prepared_data.save_to_disk(\"/mnt/DATA/bookcorpus/bert/prepared\")\n",
    "        \n",
    "        dataset=prepared_data['train']\n",
    "        tokenizer=BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        def extract_first_sentences(examples):\n",
    "            for i, input_ids in enumerate(examples[\"input_ids\"]):\n",
    "                idx=input_ids.index(tokenizer.sep_token_id)\n",
    "                examples[\"input_ids\"][i]=input_ids[:idx+1]\n",
    "                examples[\"attention_mask\"][i]=examples[\"attention_mask\"][i][:idx+1]\n",
    "                examples[\"token_type_ids\"][i]=examples[\"token_type_ids\"][i][:idx+1]\n",
    "                examples[\"special_tokens_mask\"][i]=examples[\"special_tokens_mask\"][i][:idx+1]\n",
    "            return examples\n",
    "\n",
    "        # filter points from dataset with next_sentence_label == 0\n",
    "        with accelerator.main_process_first():\n",
    "            nsp_zero=dataset.filter(lambda examples: [x==0 for x in examples[\"next_sentence_label\"]], batched=True, num_proc=args.preprocessing_num_workers, keep_in_memory=True)\n",
    "            nsp_zero.save_to_disk(\"/mnt/DATA/bookcorpus/bert/nsp_zero\")\n",
    "                \n",
    "        # filter points from dataset with next_sentence_label == 1\n",
    "        with accelerator.main_process_first():\n",
    "                nsp_one=dataset.filter(lambda examples: [x==1 for x in examples[\"next_sentence_label\"]], batched=True, num_proc=args.preprocessing_num_workers, keep_in_memory=True)\n",
    "                nsp_one.save_to_disk(\"/mnt/DATA/bookcorpus/bert/nsp_one\")\n",
    "        \n",
    "        # extract first sentences from both datasets\n",
    "        with accelerator.main_process_first():\n",
    "            first_sent_nsp_zero=nsp_zero.map(extract_first_sentences, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=[\"next_sentence_label\", \"special_tokens_mask\"], keep_in_memory=True)\n",
    "        with accelerator.main_process_first():\n",
    "            first_sent_nsp_one=nsp_one.map(extract_first_sentences, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=[\"next_sentence_label\", \"special_tokens_mask\"], keep_in_memory=True)\n",
    "\n",
    "        # save datasets\n",
    "        first_sent_nsp_zero.save_to_disk(\"/mnt/DATA/bookcorpus/bert/first_sent_nsp_zero\")\n",
    "        first_sent_nsp_one.save_to_disk(\"/mnt/DATA/bookcorpus/bert/first_sent_nsp_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data is already preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.preprocessed:\n",
    "    logger.info(f\"Loading preprocessed dataset(s) from disk.\")\n",
    "    dataset=load_from_disk(\"/mnt/DATA/bookcorpus/bert/prepared\")\n",
    "    train_dataset=dataset[\"train\"]\n",
    "    eval_dataset=dataset[\"validation\"]\n",
    "\n",
    "    nsp_zero=load_from_disk(\"/mnt/DATA/bookcorpus/bert/nsp_one\")\n",
    "    nsp_one=load_from_disk(\"/mnt/DATA/bookcorpus/bert/nsp_zero\")\n",
    "    first_sent_nsp_zero = load_from_disk(\"/mnt/DATA/bookcorpus/bert/first_sent_nsp_zero\")\n",
    "    first_sent_nsp_one = load_from_disk(\"/mnt/DATA/bookcorpus/bert/first_sent_nsp_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the random data subset selection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Random Subset Selection\n",
    "if accelerator.is_main_process:\n",
    "    num_samples = int(round(len(train_dataset) * args.subset_fraction, 0))\n",
    "    init_subset_indices = [random.sample(list(range(len(train_dataset))), num_samples)]\n",
    "else:\n",
    "    init_subset_indices = [[]]\n",
    "accelerator.wait_for_everyone()\n",
    "broadcast_object_list(init_subset_indices)\n",
    "full_dataset=train_dataset\n",
    "subset_dataset = full_dataset.select(init_subset_indices[0])\n",
    "\n",
    "logger.info(f\"Full data has {len(full_dataset)} samples, subset data has {len(subset_dataset)} samples.\")\n",
    "# Conditional for small test subsets\n",
    "if len(train_dataset)>3:\n",
    "    # Log a few random samples from the training data\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collators and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "# This one will take care of the randomly masking the tokens.\n",
    "data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n",
    "data_collator_embd=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "logger.info(f\"Creating Data Loaders\")\n",
    "# Dataloaders creation\n",
    "warmstart_dataloader=DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "first_sent_nsp_zero_dataloader=DataLoader(\n",
    "    first_sent_nsp_zero, shuffle=False, collate_fn=data_collator_embd, batch_size=args.per_device_eval_batch_size\n",
    ")\n",
    "first_sent_nsp_one_dataloader=DataLoader(\n",
    "    first_sent_nsp_one, shuffle=False, collate_fn=data_collator_embd, batch_size=args.per_device_eval_batch_size\n",
    ")\n",
    "\n",
    "subset_dataloader=DataLoader(\n",
    "    subset_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "eval_dataloader=DataLoader(\n",
    "    eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Intializing optimizer, learning rate schedule\")\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not\n",
    "no_decay=[\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters=[\n",
    "    {\n",
    "        \"params\":[p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\":args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\":[p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer=AdamW(optimizer_grouped_parameters, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init for TPU & Init Scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n",
    "if accelerator.distributed_type==DistributedType.TPU:\n",
    "    model.tie_weights()\n",
    "\n",
    "lr_scheduler=get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.lr_max_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model for the accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Prepare model, optimizer, warmstart_dataloader, first_sent_nsp_zero_dataloader, first_sent_nsp_one_dataloader, subset_dataloader, eval_dataloader with accelerate.\")\n",
    "# Prepare everything with our `accelerator`\n",
    "model, optimizer, warmstart_dataloader, first_sent_nsp_zero_dataloader, first_sent_nsp_one_dataloader, subset_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, warmstart_dataloader, first_sent_nsp_zero_dataloader, first_sent_nsp_one_dataloader, subset_dataloader, eval_dataloader)\n",
    "\n",
    "if args.selection_strategy in ['fl', 'logdet', 'gc', 'disparity-sum']:\n",
    "    subset_strategy = SubmodStrategy(logger, args.selection_strategy,\n",
    "                                num_partitions=args.num_partitions, partition_strategy=args.partition_strategy,\n",
    "                                optimizer=args.optimizer, similarity_criterion='feature', \n",
    "                                metric='cosine', eta=1, stopIfZeroGain=False, \n",
    "                                stopIfNegativeGain=False, verbose=False, lambdaVal=1)\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "if hasattr(args.checkpointing_steps, \"isdigit\"):\n",
    "    checkpointing_steps=args.checkpointing_steps\n",
    "    if args.checkpointing_steps.isdigit():\n",
    "        checkpointing_steps=int(args.checkpointing_steps)\n",
    "else:\n",
    "    checkpointing_steps=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warmstarting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "total_batch_size=args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "main_start_time=time.time()\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num warm-start epochs = {args.num_warmstart_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    accelerator.print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\n",
    "    accelerator.load_state(args.resume_from_checkpoint)\n",
    "\n",
    "logger.info(f\"Begin the training.\")\n",
    "timing = []\n",
    "warmstart_start_time=time.time()\n",
    "for epoch in range(args.num_warmstart_epochs):\n",
    "    if epoch==0:\n",
    "        logger.info(\"Begin the warm-start\")\n",
    "    model.train()\n",
    "    for step, batch in enumerate(warmstart_dataloader):\n",
    "        start_time=time.time()\n",
    "        outputs=model(**batch)\n",
    "        loss=outputs.loss\n",
    "        logger.info(f\"Completed Steps: {1+completed_steps}; Loss: {loss.detach().float()}; lr: {lr_scheduler.get_last_lr()};\")\n",
    "        loss=loss/args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step%args.gradient_accumulation_steps==0 or step==len(warmstart_dataloader)-1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps+=1\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps%checkpointing_steps==0:\n",
    "                output_dir=f\"step_{completed_steps }\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir=os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "        if completed_steps>=args.max_train_steps:\n",
    "            break\n",
    "        timing.append([(time.time() - start_time), 0])\n",
    "    \n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs=model(**batch)\n",
    "\n",
    "        loss=outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n",
    "\n",
    "    losses=torch.cat(losses)\n",
    "    losses=losses[:len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity=math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity=float(\"inf\")\n",
    "\n",
    "    logger.info(f\"Steps {completed_steps}: perplexity: {perplexity}\")\n",
    "    if epoch==args.num_warmstart_epochs-1:\n",
    "        logger.info(\"End the warm-start\")\n",
    "# Save the state after warm-start\n",
    "output_dir=f\"after_warmstart_step_{completed_steps}\"\n",
    "if args.output_dir is not None:\n",
    "    output_dir=os.path.join(args.output_dir, output_dir)\n",
    "accelerator.save_state(output_dir)\n",
    "warmstart_end_time=time.time()\n",
    "logger.info(f\"Completed warm-start in {warmstart_end_time-warmstart_start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data subselection & Restructuring for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_nsp_zero=[]\n",
    "probs_nsp_one=[]\n",
    "greedyList_nsp_zero=[]\n",
    "greedyList_nsp_one=[]\n",
    "gains_nsp_zero=[]\n",
    "gains_nsp_one=[]\n",
    "\n",
    "if (args.num_warmstart_epochs!=0) or (args.resume_from_checkpoint):\n",
    "    logger.info(f\"Beginning the subset selection after warm-start or resuming from checkpoint\")\n",
    "    start_time=time.time()\n",
    "    if args.selection_strategy == 'Random-Online':\n",
    "        if accelerator.is_main_process:\n",
    "            subset_indices_nsp_zero = [random.sample(list(range(len(first_sent_nsp_zero))), math.floor(num_samples/2))]\n",
    "            subset_indices_nsp_one = [random.sample(list(range(len(first_sent_nsp_one))), math.ceil(num_samples/2))]\n",
    "        else:\n",
    "            subset_indices_nsp_zero = [[]]\n",
    "            subset_indices_nsp_one = [[]]\n",
    "    elif args.selection_strategy in [\"fl\", \"logdet\", \"gc\", \"disparity-min\"]:\n",
    "        logger.info(f\"Performing Subset selection for NSP class 0\")\n",
    "        pbar=tqdm(range(len(first_sent_nsp_zero_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "        model.eval()\n",
    "        representations_nsp_zero=[]\n",
    "        batch_indices_nsp_zero=[]\n",
    "        total_cnt=0\n",
    "        total_storage=0\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model=accelerator.unwrap_model(model)\n",
    "        bert_model=unwrapped_model.bert\n",
    "        bert_model=accelerator.prepare(bert_model)\n",
    "        representations_start_time=time.time()\n",
    "        for step, batch in enumerate(first_sent_nsp_zero_dataloader):\n",
    "            with torch.no_grad():\n",
    "                output=bert_model(**batch, output_hidden_states=True)\n",
    "            embeddings=output[\"hidden_states\"][args.layer_for_similarity_computation]\n",
    "            mask=(batch['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float())\n",
    "            mask1=((batch['token_type_ids'].unsqueeze(-1).expand(embeddings.size()).float())==0)\n",
    "            mask=mask*mask1\n",
    "            mean_pooled=torch.sum(embeddings*mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "            mean_pooled = accelerator.gather(mean_pooled)\n",
    "            total_cnt += mean_pooled.size(0)\n",
    "            if accelerator.is_main_process:\n",
    "                mean_pooled = mean_pooled.cpu()\n",
    "                total_storage += sys.getsizeof(mean_pooled.storage())\n",
    "                representations_nsp_zero.append(mean_pooled)\n",
    "            pbar.update(1)\n",
    "        if accelerator.is_main_process:\n",
    "            representations_nsp_zero=torch.cat(representations_nsp_zero, dim=0)\n",
    "            representations_nsp_zero=representations_nsp_zero[:len(first_sent_nsp_zero)]\n",
    "            total_storage += sys.getsizeof(representations_nsp_zero.storage())\n",
    "            representations_nsp_zero=representations_nsp_zero.numpy()\n",
    "            logger.info('Representations(NSP Class 0) Size: {}, Total number of samples: {}'.format(total_storage/(1024 * 1024), total_cnt))\n",
    "            batch_indices_nsp_zero=list(range(len(first_sent_nsp_zero)))\n",
    "            logger.info('Length of indices: {}'.format(len(batch_indices_nsp_zero)))\n",
    "            logger.info('Representations(NSP Class 0) gathered. Shape of representations: {}. Length of indices: {}'.format(representations_nsp_zero.shape, len(batch_indices_nsp_zero)))\n",
    "        logger.info(f\"Representations(NSP Class 0) computed in {time.time()-representations_start_time} seconds\")\n",
    "        if accelerator.is_main_process:\n",
    "            partition_indices_nsp_zero, greedyIdx_nsp_zero, gains_nsp_zero = subset_strategy.select(len(batch_indices_nsp_zero)-1, batch_indices_nsp_zero, representations_nsp_zero, parallel_processes=args.parallel_processes, return_gains=True)\n",
    "            subset_indices_nsp_zero = [[]]\n",
    "            i=0\n",
    "            for p in gains_nsp_zero:\n",
    "                greedyList_nsp_zero.append(greedyIdx_nsp_zero[i:i+len(p)])         \n",
    "                i+=len(p)\n",
    "            probs_nsp_zero=[taylor_softmax_v1(torch.from_numpy(np.array([partition_gains])/args.temperature)).numpy()[0] for partition_gains in gains_nsp_zero]\n",
    "            rng=np.random.default_rng(args.seed+completed_steps)\n",
    "            for i, partition_prob in enumerate(probs_nsp_zero):\n",
    "                partition_budget=min(math.ceil((len(partition_prob)/len(batch_indices_nsp_zero)) * math.floor(num_samples/2)), len(partition_prob)-1)\n",
    "                subset_indices_nsp_zero[0].extend(rng.choice(greedyList_nsp_zero[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "        else:\n",
    "            subset_indices_nsp_zero=[[]]\n",
    "    \n",
    "        logger.info(f\"Performing Subset selection for NSP class 1\")\n",
    "        pbar=tqdm(range(len(first_sent_nsp_one_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "        model.eval()\n",
    "        representations_nsp_one=[]\n",
    "        batch_indices_nsp_one=[]\n",
    "        total_cnt=0\n",
    "        total_storage=0\n",
    "        representations_start_time=time.time()\n",
    "        for step, batch in enumerate(first_sent_nsp_one_dataloader):\n",
    "            with torch.no_grad():\n",
    "                output=bert_model(**batch, output_hidden_states=True)\n",
    "            embeddings=output[\"hidden_states\"][args.layer_for_similarity_computation]\n",
    "            mask=(batch['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float())\n",
    "            mask1=((batch['token_type_ids'].unsqueeze(-1).expand(embeddings.size()).float())==0)\n",
    "            mask=mask*mask1\n",
    "            mean_pooled=torch.sum(embeddings*mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "            mean_pooled = accelerator.gather(mean_pooled)\n",
    "            total_cnt += mean_pooled.size(0)\n",
    "            if accelerator.is_main_process:\n",
    "                mean_pooled = mean_pooled.cpu()\n",
    "                total_storage += sys.getsizeof(mean_pooled.storage())\n",
    "                representations_nsp_one.append(mean_pooled)\n",
    "            pbar.update(1)\n",
    "        if accelerator.is_main_process:\n",
    "            representations_nsp_one=torch.cat(representations_nsp_one, dim=0)\n",
    "            representations_nsp_one=representations_nsp_one[:len(first_sent_nsp_one)]\n",
    "            total_storage += sys.getsizeof(representations_nsp_one.storage())\n",
    "            representations_nsp_one=representations_nsp_one.numpy()\n",
    "            logger.info('Representations(NSP Class 1) Size: {}, Total number of samples: {}'.format(total_storage/(1024 * 1024), total_cnt))\n",
    "            batch_indices_nsp_one=list(range(len(first_sent_nsp_one)))\n",
    "            logger.info('Length of indices: {}'.format(len(batch_indices_nsp_one)))\n",
    "            logger.info('Representations(NSP Class 1) gathered. Shape of representations: {}. Length of indices: {}'.format(representations_nsp_one.shape, len(batch_indices_nsp_one)))\n",
    "        logger.info(f\"Representations(NSP Class 1) computed in {time.time()-representations_start_time} seconds\")\n",
    "        if accelerator.is_main_process:\n",
    "            partition_indices_nsp_one, greedyIdx_nsp_one, gains_nsp_one = subset_strategy.select(len(batch_indices_nsp_one)-1, batch_indices_nsp_one, representations_nsp_one, parallel_processes=args.parallel_processes, return_gains=True)\n",
    "            subset_indices_nsp_one = [[]]\n",
    "            i=0\n",
    "            for p in gains_nsp_one:\n",
    "                greedyList_nsp_one.append(greedyIdx_nsp_one[i:i+len(p)])         \n",
    "                i+=len(p)\n",
    "            probs_nsp_one=[taylor_softmax_v1(torch.from_numpy(np.array([partition_gains])/args.temperature)).numpy()[0] for partition_gains in gains_nsp_one]\n",
    "            rng=np.random.default_rng(args.seed+completed_steps)\n",
    "            for i, partition_prob in enumerate(probs_nsp_one):\n",
    "                partition_budget=min(math.ceil((len(partition_prob)/len(batch_indices_nsp_one)) * math.ceil(num_samples/2)), len(partition_prob)-1)\n",
    "                subset_indices_nsp_one[0].extend(rng.choice(greedyList_nsp_one[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "        else:\n",
    "            subset_indices_nsp_one=[[]]\n",
    "    accelerator.wait_for_everyone()    \n",
    "    broadcast_object_list(subset_indices_nsp_zero)\n",
    "    broadcast_object_list(subset_indices_nsp_one)\n",
    "    timing.append([0, time.time()-start_time])\n",
    "    logger.info(f\"First subset selection completed. Total Time taken(including embeddings computation): {time.time()-start_time}\")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    output_file=f\"nsp_zero_subset_indices_after_step_{completed_steps}.pt\"\n",
    "    output_file=os.path.join(args.subset_dir, output_file)\n",
    "    torch.save(torch.tensor(subset_indices_nsp_zero), output_file)\n",
    "    output_file=f\"nsp_one_subset_indices_after_step_{completed_steps}.pt\"\n",
    "    output_file=os.path.join(args.subset_dir, output_file)\n",
    "    torch.save(torch.tensor(subset_indices_nsp_one), output_file)\n",
    "    output_file=f\"nsp_zero_gains_after_step_{completed_steps}.pkl\"\n",
    "    output_file=os.path.join(args.subset_dir, output_file)\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(gains_nsp_zero, f)\n",
    "    output_file=f\"nsp_one_gains_after_step_{completed_steps}.pkl\"\n",
    "    output_file=os.path.join(args.subset_dir, output_file)\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(gains_nsp_one, f)\n",
    "    output_file=f\"nsp_zero_partition_indices_after_step_{completed_steps}.pkl\"\n",
    "    output_file=os.path.join(args.subset_dir, output_file)\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(partition_indices_nsp_zero, f)\n",
    "    output_file=f\"nsp_one_partition_indices_after_step_{completed_steps}.pkl\"\n",
    "    output_file=os.path.join(args.subset_dir, output_file)\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(partition_indices_nsp_one, f)\n",
    "    output_file=f\"nsp_zero_greedy_indices_after_step_{completed_steps}.pkl\"\n",
    "    output_file=os.path.join(args.subset_dir, output_file)\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(greedyIdx_nsp_zero, f)\n",
    "    output_file=f\"nsp_one_greedy_indices_after_step_{completed_steps}.pkl\"\n",
    "    output_file=os.path.join(args.subset_dir, output_file)\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(greedyIdx_nsp_one, f)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "nsp_zero_subset_dataset=nsp_zero.select(subset_indices_nsp_zero[0])\n",
    "nsp_one_subset_dataset=nsp_one.select(subset_indices_nsp_one[0])\n",
    "# Concatenate the two datasets\n",
    "subset_dataset = concatenate_datasets([nsp_zero_subset_dataset, nsp_one_subset_dataset])\n",
    "subset_dataloader=DataLoader(\n",
    "    subset_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n",
    "subset_dataloader = accelerator.prepare(subset_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
