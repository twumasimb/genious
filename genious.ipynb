{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datasets\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed, broadcast_object_list\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizerFast,\n",
    "    BertForPreTraining,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from selectionstrategies import SubmodStrategy\n",
    "from accelerate import InitProcessGroupKwargs\n",
    "from selectionstrategies.helper_fns import taylor_softmax_v1\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Parameters for args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.log_dir = \"./path/to/log_dir\"  # Update with your path\n",
    "        self.subset_dir = \"./path/to/subset_dir\"  # Update with your path\n",
    "        self.output_dir = \"./path/to/output_dir\"  # Update with your path\n",
    "        self.preprocessed = False  # Set to True if preprocessed\n",
    "        self.load_data_from_disk = None  # Set to True if loading data from disk\n",
    "        self.data_directory = None  # Update with your path if load_data_from_disk is True\n",
    "        self.dataset_name = None  # Specify dataset name if needed\n",
    "        self.dataset_config_name = None  # Specify dataset config name if needed\n",
    "        self.train_file = None  # Path to your training file if applicable\n",
    "        self.validation_file = None  # Path to your validation file if applicable\n",
    "        self.validation_split_percentage = 5  # Update this as per your requirement\n",
    "        self.pad_to_max_length = False  # Set to True to pad to max_length\n",
    "        self.model_name_or_path = None  # Update with your model name or path\n",
    "        self.config_name = None  # Specify config name or path if not the same as model_name\n",
    "        self.hidden_size = 768\n",
    "        self.num_hidden_layers = 12\n",
    "        self.num_attention_heads = 12\n",
    "        self.intermediate_size = 3072\n",
    "        self.tokenizer_name = None  # Specify tokenizer name or path if not the same as model_name\n",
    "        self.vocab_size = 30522\n",
    "        self.use_slow_tokenizer = False  # Set to True to use a slow tokenizer\n",
    "        self.per_device_train_batch_size = 8\n",
    "        self.per_device_eval_batch_size = 8\n",
    "        self.learning_rate = 1e-4\n",
    "        self.lr_max_steps = 1000000\n",
    "        self.weight_decay = 0.01\n",
    "        self.max_train_steps = 250000\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.lr_scheduler_type = \"linear\"  # Adjust as needed, ensure it's a valid choice\n",
    "        self.num_warmup_steps = 10000\n",
    "        self.seed = None  # Specify a seed for reproducible training if needed\n",
    "        self.max_seq_length = 128\n",
    "        self.line_by_line = False  # Set to True if handling distinct lines as sequences\n",
    "        self.preprocessing_num_workers = 96\n",
    "        self.preprocess_batch_size = None  # Specify batch size during preprocessing if needed\n",
    "        self.overwrite_cache = False  # Set to True to overwrite cached sets\n",
    "        self.mlm_probability = 0.15\n",
    "        self.short_seq_prob = 0.1\n",
    "        self.nsp_probability = 0.5\n",
    "        self.subset_fraction = 0.25\n",
    "        self.selection_strategy = 'fl'  # Update with your strategy\n",
    "        self.optimizer = \"LazyGreedy\"  # Update with your optimizer\n",
    "        self.select_every = 25000\n",
    "        self.partition_strategy = \"random\"  # Update with your strategy\n",
    "        self.layer_for_similarity_computation = 9\n",
    "        self.num_partitions = 5000\n",
    "        self.parallel_processes = 96\n",
    "        self.num_warmstart_epochs = 1\n",
    "        self.checkpointing_steps = None  # Specify if needed\n",
    "        self.resume_from_checkpoint = None  # Specify path if resuming from checkpoint\n",
    "        self.temperature = 1.0  # Update if using a different temperature\n",
    "        self.main_process_port = None  # Update with your port\n",
    "        self.visible_gpus = '0'  # Update with your GPU(s)\n",
    "        self.init_split = None  # Specify to use a smaller portion of the data. Example: 20 means 80% of the data is used\n",
    "\n",
    "# Instantiate the args\n",
    "args = Args()\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "\n",
    "# The following can be modified as needed based on your requirements\n",
    "args.preprocessed = False\n",
    "args.visible_gpus = '1'  # Assuming you want to use the first GPU; adjust as needed\n",
    "args.main_process_port = 29500  # Example port; adjust as needed\n",
    "args.log_dir = f\"./output/logs/genious{timestamp}/\"\n",
    "args.model_dir = f\"./output/models/genious{timestamp}/\"\n",
    "args.subset_dir = f\"./output/subsets/genious{timestamp}/\"\n",
    "args.load_data_from_disk = None\n",
    "args.data_directory = \"./data\"\n",
    "args.tokenizer_name = \"bert-base-uncased\"\n",
    "args.preprocess_batch_size = 100\n",
    "args.per_device_train_batch_size = 4\n",
    "args.per_device_eval_batch_size = 4\n",
    "args.learning_rate = 1e-4\n",
    "args.lr_max_steps = 1000\n",
    "args.weight_decay = 0.01\n",
    "args.max_train_steps = 10\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.num_warmup_steps = 10\n",
    "args.output_dir = args.model_dir\n",
    "args.model_type = \"bert\"\n",
    "args.max_seq_length = 64\n",
    "args.preprocessing_num_workers = 10\n",
    "args.mlm_probability = 0.15\n",
    "args.short_seq_prob = 0.1\n",
    "args.nsp_probability = 0.5\n",
    "args.subset_fraction = 0.25\n",
    "args.update_losses_every = 1250\n",
    "args.checkpointing_steps = 250 \n",
    "args.init_split = 95 # ignore 95% of the data\n",
    "args.dataset_name = \"Salesforce/wikitext\"\n",
    "args.dataset_config_name = \"wikitext-2-raw-v1\", \n",
    "args.seed = 42\n",
    "\n",
    "\n",
    "# Set CUDA visible devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
    "\n",
    "debug = True\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "os.makedirs(args.model_dir, exist_ok=True)\n",
    "os.makedirs(args.subset_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Logger & Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger=get_logger(__name__)\n",
    "init_process_group=InitProcessGroupKwargs(timeout=datetime.timedelta(seconds=75000))\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "accelerator=Accelerator(kwargs_handlers=[init_process_group])\n",
    "# Make one log on every process with the configuration for debugging\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(args.log_dir,\"train_logs.log\"),\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "# logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "if args.preprocessed == False:\n",
    "    logger.info(f\"Data is not preprocessed.\")\n",
    "    logger.info(f\"Loading the data.\")\n",
    "    if args.load_data_from_disk is not None:\n",
    "        if args.data_directory is not None:\n",
    "            raw_datasets=load_from_disk(args.data_directory)\n",
    "            if \"validation\" not in raw_datasets.keys():\n",
    "                raw_datasets=raw_datasets[\"train\"].train_test_split(test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "                raw_datasets=datasets.DatasetDict({\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})\n",
    "    elif args.dataset_name is not None:\n",
    "        raw_datasets=load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "        if \"validaton\" not in raw_datasets.keys() and args.init_split is not None:\n",
    "            raw_datasets=raw_datasets[\"train\"].train_test_split(test_size=(args.init_split/100), shuffle=False)\n",
    "            raw_datasets=raw_datasets[\"train\"].train_test_split(test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "            raw_datasets=datasets.DatasetDict({\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})\n",
    "        elif \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets=raw_datasets.train_test_split(test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "            raw_datasets=datasets.DatasetDict({\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})\n",
    "    else:\n",
    "        data_files={}\n",
    "        if args.train_file is not None:\n",
    "            data_files['train']=args.train_file\n",
    "        if args.validation_file is not None:\n",
    "            data_files['validation']=args.validation_file\n",
    "        extension=args.train_file.split(\".\")[-1]\n",
    "        if extension=='txt':\n",
    "            extension='text'\n",
    "        raw_datasets=load_dataset(extension, data_files=data_files)\n",
    "        if \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets=raw_datasets[\"train\"].train_test_split(test_size=(args.validation_split_percentage/100), shuffle=False)\n",
    "            raw_datasets=datasets.DatasetDict({\"train\": raw_datasets[\"train\"], \"validation\": raw_datasets[\"test\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1743\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 92\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/twumasimb/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/twumasimb/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/twumasimb/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/twumasimb/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 30522. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "logger.info(f\"Loading the model configuration.\")\n",
    "if args.config_name:\n",
    "    config=BertConfig.from_pretrained(args.config_name)\n",
    "elif args.model_name_or_path:\n",
    "    config=BertConfig.from_pretrained(args.model_name_or_path)\n",
    "else:\n",
    "    config=BertConfig(\n",
    "        vocab_size=args.vocab_size,\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "        intermediate_size=args.intermediate_size,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        position_embedding_type=\"absolute\",\n",
    "    )\n",
    "\n",
    "logger.info(f\"Loading the tokenizer.\")\n",
    "if args.tokenizer_name:\n",
    "    tokenizer=BertTokenizerFast.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer=BertTokenizerFast.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "logger.info(f\"Initializing Model.\")\n",
    "if args.model_name_or_path:\n",
    "    model=BertForPreTraining.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\"in args.model_name_or_path),\n",
    "        config=config\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training a new model from scratch\")\n",
    "    model=BertForPreTraining(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the datasets\n",
    "#First we tokenize all the texts\n",
    "if args.preprocessed == False:\n",
    "    column_names=raw_datasets['train'].column_names\n",
    "    text_column_name=\"text\" if \"text\" in column_names else column_names[0]\n",
    "else:\n",
    "    column_names=[\"text\"]\n",
    "    text_column_name=\"text\"\n",
    "\n",
    "if args.max_seq_length is None:\n",
    "    max_seq_length=tokenizer.model_max_length\n",
    "    if max_seq_length>1024:\n",
    "        logger.warning(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}).\"\n",
    "            \"Picking 1024 instead. You can change the default value by passing --max_seq_length xxx\"\n",
    "        )\n",
    "        max_seq_length=1024\n",
    "else:\n",
    "    if args.max_seq_length>tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the \"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length=min(args.max_seq_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main data processing function that will concatenate all texts from our dataset and generate chunks of \n",
    "#max_seq_length.\n",
    "\n",
    "def group_texts(examples, idx, split, tokenized_datasets):\n",
    "    # Account for [CLS], [SEP], [SEP]\n",
    "    max_num_tokens=max_seq_length-3\n",
    "    # We *usually* want to fill up the entire sequence since we are padding\n",
    "    # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "    # computation. However, we *sometimes*\n",
    "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "    # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "    # The `target_seq_length` is just a rough target however, whereas\n",
    "    # `max_seq_length` is a hard limit.\n",
    "    target_seq_length=max_num_tokens\n",
    "    if random.random()<args.short_seq_prob:\n",
    "        target_seq_length=random.randint(2, max_num_tokens)\n",
    "    # We DON'T just concatenate all of the tokens from a document into a long\n",
    "    # sequence and choose an arbitrary split point because this would make the\n",
    "    # next sentence prediction task too easy. Instead, we split the input into\n",
    "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "    # input.\n",
    "    result={k: [] for k, v in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "    result['next_sentence_label']=[]\n",
    "    current_chunk=[]\n",
    "    current_length=0\n",
    "    i=0 \n",
    "    while i<len(idx):\n",
    "        segment={k: examples[k][i][1:-1] for k in examples.keys()}\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment['input_ids'])\n",
    "        if i==len(idx)-1 or current_length>=target_seq_length:\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end=1\n",
    "                if len(current_chunk)>=2:\n",
    "                    a_end=random.randint(1, len(current_chunk)-1)\n",
    "                tokens_a={k: [] for k, t in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                for j in range(a_end):\n",
    "                    for k, v in current_chunk[j].items():\n",
    "                        tokens_a[k].extend(v)\n",
    "\n",
    "                tokens_b={k: [] for k, t in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                # Random next\n",
    "                is_random_next=False\n",
    "                if len(current_chunk)==1 or random.random()<args.nsp_probability:\n",
    "                    is_random_next=True\n",
    "                    target_b_length=target_seq_length-len(tokens_a[\"input_ids\"])\n",
    "                    # This should rarely go for more than one iteration for large\n",
    "                    # corpora. However, just to be careful, we try to make sure that\n",
    "                    # the random document is not the same as the document\n",
    "                    # we're processing.\n",
    "                    for _ in range(10):\n",
    "                        random_segment_index=random.randint(0, len(tokenized_datasets[split])-len(idx)-1)\n",
    "                        if (random_segment_index-len(idx) not in idx) and (random_segment_index+len(idx) not in idx):\n",
    "                            break\n",
    "\n",
    "                    random_start=random.randint(0, len(idx)-1)\n",
    "                    for j in range(random_start, len(idx)):\n",
    "                        for k, v in {k: tokenized_datasets[split][random_segment_index+j][k][1:-1] for k in examples.keys()}.items():\n",
    "                            tokens_b[k].extend(v)\n",
    "                        if len(tokens_b['input_ids'])>=target_b_length:\n",
    "                            break\n",
    "                    # We didn't actually use these segments so we \"put them back\" so\n",
    "                    # they don't go to waste.\n",
    "                    num_unused_segments=len(current_chunk)-a_end\n",
    "                    i-=num_unused_segments\n",
    "                # Actual next\n",
    "                else:\n",
    "                    is_random_next=False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        for k, v in current_chunk[j].items():\n",
    "                            tokens_b[k].extend(v)\n",
    "\n",
    "                while True:\n",
    "                    total_length=len(tokens_a['input_ids'])+len(tokens_b['input_ids'])\n",
    "                    if total_length<=max_num_tokens:\n",
    "                        break\n",
    "                    trunc_tokens= tokens_a if len(tokens_a['input_ids'])>len(tokens_b['input_ids']) else tokens_b\n",
    "                    # We want to sometimes truncate from the front and sometimes from the\n",
    "                    # back to add more randomness and avoid biases.\n",
    "                    if random.random()<0.5:\n",
    "                        for k in trunc_tokens.keys():\n",
    "                            del trunc_tokens[k][0]\n",
    "                    else:\n",
    "                        for k in trunc_tokens.keys():\n",
    "                            trunc_tokens[k].pop()\n",
    "                inp={k: v[:-1] for k, v in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                for k, v in tokens_a.items():\n",
    "                    inp[k].extend(v)\n",
    "                SEP={k: v[1:] for k, v in tokenizer(\"\", return_special_tokens_mask=True).items()}\n",
    "                for k, v in SEP.items():\n",
    "                    inp[k].extend(v)\n",
    "                tokens_b['token_type_ids']=list(map(lambda x: 1, tokens_b['token_type_ids']))\n",
    "                for k, v in SEP.items():\n",
    "                    tokens_b[k].extend(v)\n",
    "                tokens_b['token_type_ids'][-1]=1\n",
    "                for k, v in tokens_b.items():\n",
    "                    inp[k].extend(v)\n",
    "                inp['next_sentence_label']=int(is_random_next)\n",
    "                for k, v in inp.items():\n",
    "                    result[k].append(v)\n",
    "            current_chunk=[]\n",
    "            current_length=0\n",
    "        i+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed0d78d5a814b69bf2566bf988a34f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on every text in dataset (num_proc=10):   0%|          | 0/1743 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4272168369784294949496f9125e61cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on every text in dataset (num_proc=10):   0%|          | 0/92 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b420ac1f7f845f7b3d90559f9646900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping Train texts in chunks of 64 (num_proc=10):   0%|          | 0/1743 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48daa86fd6a7455185ce377a88069d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping Validation texts in chunks of 64 (num_proc=10):   0%|          | 0/92 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a474c679a71b46118dcd9d2ea6d94cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0f5cc6e85f4b33a56074886528334e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/twumasimb/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/twumasimb/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/twumasimb/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/twumasimb/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8ef04b85b64a82b844b7ea48944b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=10):   0%|          | 0/874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de12b6b902842f0a42d6c79f5f3cda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a78c8e94374f7a9651784a810c0721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=10):   0%|          | 0/874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a4c17d964d4452a9c4fb302f727d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82e960013b44e8f8c548fb7cebc46fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d28992dd514fde9b3dffe22ae3f7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82adfbb61b6e46b5a52164464045af9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc097b002ee44f78be969993383ef4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.preprocessed == False:\n",
    "    logger.info(f\"Beginning Tokenization.\")\n",
    "    if args.line_by_line == True:\n",
    "        #when using line_by_line, we just tokenize each non-empty line.\n",
    "        padding=\"max_length\" if args.pad_to_max_length==True else False\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            #remove empty lines\n",
    "            examples[text_column_name]=[\n",
    "                line for line in examples[text_column_name] if len(line)>0 and not line.isspace()\n",
    "            ]\n",
    "            return tokenizer(\n",
    "                examples[text_column_name],\n",
    "                padding=padding,\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                #we use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "                #receives the `special_tokens_mask`\n",
    "                return_special_tokens_mask=True\n",
    "            )\n",
    "        \n",
    "        with accelerator.main_process_first():\n",
    "            tokenized_datasets=raw_datasets.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=args.preprocessing_num_workers,\n",
    "                remove_columns=[text_column_name],\n",
    "                load_from_cache_file=not args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on dataset line by line\",\n",
    "            )\n",
    "        train_dataset=tokenized_datasets[\"train\"]\n",
    "        eval_dataset=tokenized_datasets[\"validation\"]\n",
    "    else:\n",
    "        # otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "        # We use `return_special_tokens=True` because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "        with accelerator.main_process_first():\n",
    "            tokenized_datasets=raw_datasets.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=args.preprocessing_num_workers,\n",
    "                remove_columns=column_names,\n",
    "                load_from_cache_file=not args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on every text in dataset\",\n",
    "            )\n",
    "        \n",
    "        # Note that with `batched=True`, this map processes 1000 texts together, so group_texts throws away a \n",
    "        # remainder for each of those groups of 1000 texts. You can adjust that batch_size here, but a higher value\n",
    "        # might be slower to preprocess.\n",
    "        #\n",
    "        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "        train_dataset=tokenized_datasets[\"train\"]\n",
    "        eval_dataset=tokenized_datasets[\"validation\"]\n",
    "        logger.info(f\"Grouping the tokenized dataset into chunks of {max_seq_length}.\")\n",
    "        with accelerator.main_process_first():\n",
    "            train_dataset=train_dataset.map(\n",
    "                group_texts,\n",
    "                fn_kwargs={'split': 'train', 'tokenized_datasets': tokenized_datasets},\n",
    "                batched=True,\n",
    "                batch_size=args.preprocess_batch_size,\n",
    "                num_proc=args.preprocessing_num_workers,\n",
    "                load_from_cache_file=not args.overwrite_cache,\n",
    "                with_indices=True,\n",
    "                desc=f\"Grouping Train texts in chunks of {max_seq_length}\",\n",
    "            )\n",
    "        with accelerator.main_process_first():\n",
    "            eval_dataset=eval_dataset.map(\n",
    "                group_texts,\n",
    "                fn_kwargs={'split': 'validation', 'tokenized_datasets': tokenized_datasets},\n",
    "                batched=True,\n",
    "                batch_size=args.preprocess_batch_size,\n",
    "                num_proc=args.preprocessing_num_workers,\n",
    "                load_from_cache_file=not args.overwrite_cache,\n",
    "                with_indices=True,\n",
    "                desc=f\"Grouping Validation texts in chunks of {max_seq_length}\",\n",
    "            )\n",
    "\n",
    "        prepared_data  = datasets.DatasetDict({\"train\": train_dataset, \"validation\": eval_dataset})\n",
    "        prepared_data.save_to_disk(\"./data/bookcorpus/bert/prepared\")\n",
    "        \n",
    "        dataset=prepared_data['train']\n",
    "        tokenizer=BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        def extract_first_sentences(examples):\n",
    "            for i, input_ids in enumerate(examples[\"input_ids\"]):\n",
    "                idx=input_ids.index(tokenizer.sep_token_id)\n",
    "                examples[\"input_ids\"][i]=input_ids[:idx+1]\n",
    "                examples[\"attention_mask\"][i]=examples[\"attention_mask\"][i][:idx+1]\n",
    "                examples[\"token_type_ids\"][i]=examples[\"token_type_ids\"][i][:idx+1]\n",
    "                examples[\"special_tokens_mask\"][i]=examples[\"special_tokens_mask\"][i][:idx+1]\n",
    "            return examples\n",
    "\n",
    "        # filter points from dataset with next_sentence_label == 0\n",
    "        with accelerator.main_process_first():\n",
    "            nsp_zero=dataset.filter(lambda examples: [x==0 for x in examples[\"next_sentence_label\"]], batched=True, num_proc=args.preprocessing_num_workers, keep_in_memory=True)\n",
    "            nsp_zero.save_to_disk(\"./data/bookcorpus/bert/nsp_zero\")\n",
    "                \n",
    "        # filter points from dataset with next_sentence_label == 1\n",
    "        with accelerator.main_process_first():\n",
    "                nsp_one=dataset.filter(lambda examples: [x==1 for x in examples[\"next_sentence_label\"]], batched=True, num_proc=args.preprocessing_num_workers, keep_in_memory=True)\n",
    "                nsp_one.save_to_disk(\"./data/bookcorpus/bert/nsp_one\")\n",
    "        \n",
    "        # extract first sentences from both datasets\n",
    "        with accelerator.main_process_first():\n",
    "            first_sent_nsp_zero=nsp_zero.map(extract_first_sentences, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=[\"next_sentence_label\", \"special_tokens_mask\"], keep_in_memory=True)\n",
    "            first_sent_nsp_zero.save_to_disk(\"./data/bookcorpus/bert/first_sent_nsp_zero\")\n",
    "        with accelerator.main_process_first():\n",
    "            first_sent_nsp_one=nsp_one.map(extract_first_sentences, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=[\"next_sentence_label\", \"special_tokens_mask\"], keep_in_memory=True)\n",
    "            first_sent_nsp_one.save_to_disk(\"./data/bookcorpus/bert/first_sent_nsp_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data is already preprocessed, load it them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.preprocessed == True:\n",
    "    logger.info(f\"Loading preprocessed dataset(s) from disk.\")\n",
    "    dataset=load_from_disk(\"./data/bookcorpus/bert/prepared\")\n",
    "    train_dataset=dataset[\"train\"]\n",
    "    eval_dataset=dataset[\"validation\"]\n",
    "\n",
    "    nsp_zero=load_from_disk(\"./data/bookcorpus/bert/nsp_one\")\n",
    "    nsp_one=load_from_disk(\"./data/bookcorpus/bert/nsp_zero\")\n",
    "    first_sent_nsp_zero = load_from_disk(\"./data/bookcorpus/bert/first_sent_nsp_zero\")\n",
    "    first_sent_nsp_one = load_from_disk(\"./data/bookcorpus/bert/first_sent_nsp_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the random data subset selection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Random Subset Selection\n",
    "if accelerator.is_main_process:\n",
    "    num_samples = int(round(len(train_dataset) * args.subset_fraction, 0))\n",
    "    init_subset_indices = [random.sample(list(range(len(train_dataset))), num_samples)]\n",
    "else:\n",
    "    init_subset_indices = [[]]\n",
    "accelerator.wait_for_everyone()\n",
    "broadcast_object_list(init_subset_indices)\n",
    "full_dataset=train_dataset\n",
    "subset_dataset = full_dataset.select(init_subset_indices[0])\n",
    "\n",
    "logger.info(f\"Full data has {len(full_dataset)} samples, subset data has {len(subset_dataset)} samples.\")\n",
    "# Conditional for small test subsets\n",
    "if len(train_dataset)>3:\n",
    "    # Log a few random samples from the training data\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collators and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "# This one will take care of the randomly masking the tokens.\n",
    "data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n",
    "data_collator_embd=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "logger.info(f\"Creating Data Loaders\")\n",
    "# Dataloaders creation\n",
    "warmstart_dataloader=DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "first_sent_nsp_zero_dataloader=DataLoader(\n",
    "    first_sent_nsp_zero, shuffle=False, collate_fn=data_collator_embd, batch_size=args.per_device_eval_batch_size\n",
    ")\n",
    "first_sent_nsp_one_dataloader=DataLoader(\n",
    "    first_sent_nsp_one, shuffle=False, collate_fn=data_collator_embd, batch_size=args.per_device_eval_batch_size\n",
    ")\n",
    "\n",
    "subset_dataloader=DataLoader(\n",
    "    subset_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "eval_dataloader=DataLoader(\n",
    "    eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Intializing optimizer, learning rate schedule\")\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not\n",
    "no_decay=[\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters=[\n",
    "    {\n",
    "        \"params\":[p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\":args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\":[p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer=AdamW(optimizer_grouped_parameters, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init for TPU & Init Scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n",
    "if accelerator.distributed_type==DistributedType.TPU:\n",
    "    model.tie_weights()\n",
    "\n",
    "lr_scheduler=get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.lr_max_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model for the accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Prepare model, optimizer, warmstart_dataloader, first_sent_nsp_zero_dataloader, first_sent_nsp_one_dataloader, subset_dataloader, eval_dataloader with accelerate.\")\n",
    "# Prepare everything with our `accelerator`\n",
    "model, optimizer, warmstart_dataloader, first_sent_nsp_zero_dataloader, first_sent_nsp_one_dataloader, subset_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, warmstart_dataloader, first_sent_nsp_zero_dataloader, first_sent_nsp_one_dataloader, subset_dataloader, eval_dataloader)\n",
    "\n",
    "if args.selection_strategy in ['fl', 'logdet', 'gc', 'disparity-sum']:\n",
    "    subset_strategy = SubmodStrategy(logger, args.selection_strategy,\n",
    "                                num_partitions=args.num_partitions, partition_strategy=args.partition_strategy,\n",
    "                                optimizer=args.optimizer, similarity_criterion='feature', \n",
    "                                metric='cosine', eta=1, stopIfZeroGain=False, \n",
    "                                stopIfNegativeGain=False, verbose=False, lambdaVal=1)\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "if hasattr(args.checkpointing_steps, \"isdigit\"):\n",
    "    checkpointing_steps=args.checkpointing_steps\n",
    "    if args.checkpointing_steps.isdigit():\n",
    "        checkpointing_steps=int(args.checkpointing_steps)\n",
    "else:\n",
    "    checkpointing_steps=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warmstarting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697778488c7b475bac99aab77aedee19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train!\n",
    "print(f\"Training on device: {accelerator.device}\")\n",
    "total_batch_size=args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "main_start_time=time.time()\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num warm-start epochs = {args.num_warmstart_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint != None:\n",
    "    accelerator.print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\n",
    "    accelerator.load_state(args.resume_from_checkpoint)\n",
    "\n",
    "logger.info(f\"Begin the training.\")\n",
    "timing = []\n",
    "warmstart_start_time=time.time()\n",
    "for epoch in range(args.num_warmstart_epochs):\n",
    "    if epoch==0:\n",
    "        logger.info(\"Begin the warm-start\")\n",
    "    model.train()\n",
    "    for step, batch in enumerate(warmstart_dataloader):\n",
    "        start_time=time.time()\n",
    "        outputs=model(**batch)\n",
    "        loss=outputs.loss\n",
    "        logger.info(f\"Completed Steps: {1+completed_steps}; Loss: {loss.detach().float()}; lr: {lr_scheduler.get_last_lr()};\")\n",
    "        loss=loss/args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step%args.gradient_accumulation_steps==0 or step==len(warmstart_dataloader)-1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps+=1\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps%checkpointing_steps==0:\n",
    "                output_dir=f\"step_{completed_steps }\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir=os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "        if completed_steps>=args.max_train_steps:\n",
    "            break\n",
    "        timing.append([(time.time() - start_time), 0])\n",
    "    \n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs=model(**batch)\n",
    "\n",
    "        loss=outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n",
    "\n",
    "    losses=torch.cat(losses)\n",
    "    losses=losses[:len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity=math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity=float(\"inf\")\n",
    "\n",
    "    logger.info(f\"Steps {completed_steps}: perplexity: {perplexity}\")\n",
    "    if epoch==args.num_warmstart_epochs-1:\n",
    "        logger.info(\"End the warm-start\")\n",
    "# Save the state after warm-start\n",
    "output_dir=f\"after_warmstart_step_{completed_steps}\"\n",
    "if args.output_dir is not None:\n",
    "    output_dir=os.path.join(args.output_dir, output_dir)\n",
    "accelerator.save_state(output_dir)\n",
    "warmstart_end_time=time.time()\n",
    "logger.info(f\"Completed warm-start in {warmstart_end_time-warmstart_start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data subselection & Restructuring for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d9709c7ba9451694ff2f6ad0a5bada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition indices and partion\n",
      "Starting Pool parallel process!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 216/216 [00:00<00:00, 1463.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool Parallel process ended.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, partition_prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(probs_nsp_zero):\n\u001b[1;32m     67\u001b[0m         partition_budget\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil((\u001b[38;5;28mlen\u001b[39m(partition_prob)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch_indices_nsp_zero)) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor(num_samples\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)), \u001b[38;5;28mlen\u001b[39m(partition_prob)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m         subset_indices_nsp_zero[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(\u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgreedyList_nsp_zero\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_budget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_prob\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     subset_indices_nsp_zero\u001b[38;5;241m=\u001b[39m[[]]\n",
      "File \u001b[0;32m_generator.pyx:719\u001b[0m, in \u001b[0;36mnumpy.random._generator.Generator.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "probs_nsp_zero=[]\n",
    "probs_nsp_one=[]\n",
    "greedyList_nsp_zero=[]\n",
    "greedyList_nsp_one=[]\n",
    "gains_nsp_zero=[]\n",
    "gains_nsp_one=[]\n",
    "\n",
    "if (args.num_warmstart_epochs!=0) or (args.resume_from_checkpoint != None):\n",
    "    logger.info(f\"Beginning the subset selection after warm-start or resuming from checkpoint\")\n",
    "    start_time=time.time()\n",
    "    if args.selection_strategy == 'Random-Online':\n",
    "        if accelerator.is_main_process:\n",
    "            subset_indices_nsp_zero = [random.sample(list(range(len(first_sent_nsp_zero))), math.floor(num_samples/2))]\n",
    "            subset_indices_nsp_one = [random.sample(list(range(len(first_sent_nsp_one))), math.ceil(num_samples/2))]\n",
    "        else:\n",
    "            subset_indices_nsp_zero = [[]]\n",
    "            subset_indices_nsp_one = [[]]\n",
    "    elif args.selection_strategy in [\"fl\", \"logdet\", \"gc\", \"disparity-min\"]:\n",
    "        logger.info(f\"Performing Subset selection for NSP class 0\")\n",
    "        pbar=tqdm(range(len(first_sent_nsp_zero_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "        model.eval()\n",
    "        representations_nsp_zero=[]\n",
    "        batch_indices_nsp_zero=[]\n",
    "        total_cnt=0\n",
    "        total_storage=0\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model=accelerator.unwrap_model(model)\n",
    "        bert_model=unwrapped_model.bert\n",
    "        bert_model=accelerator.prepare(bert_model)\n",
    "        representations_start_time=time.time()\n",
    "        for step, batch in enumerate(first_sent_nsp_zero_dataloader):\n",
    "            with torch.no_grad():\n",
    "                output=bert_model(**batch, output_hidden_states=True)\n",
    "            embeddings=output[\"hidden_states\"][args.layer_for_similarity_computation]\n",
    "            mask=(batch['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float())\n",
    "            mask1=((batch['token_type_ids'].unsqueeze(-1).expand(embeddings.size()).float())==0)\n",
    "            mask=mask*mask1\n",
    "            mean_pooled=torch.sum(embeddings*mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "            mean_pooled = accelerator.gather(mean_pooled)\n",
    "            total_cnt += mean_pooled.size(0)\n",
    "            if accelerator.is_main_process:\n",
    "                mean_pooled = mean_pooled.cpu()\n",
    "                total_storage += sys.getsizeof(mean_pooled.storage())\n",
    "                representations_nsp_zero.append(mean_pooled)\n",
    "            pbar.update(1)\n",
    "        if accelerator.is_main_process:\n",
    "            representations_nsp_zero=torch.cat(representations_nsp_zero, dim=0)\n",
    "            representations_nsp_zero=representations_nsp_zero[:len(first_sent_nsp_zero)]\n",
    "            total_storage += sys.getsizeof(representations_nsp_zero.storage())\n",
    "            representations_nsp_zero=representations_nsp_zero.numpy()\n",
    "            logger.info('Representations(NSP Class 0) Size: {}, Total number of samples: {}'.format(total_storage/(1024 * 1024), total_cnt))\n",
    "            batch_indices_nsp_zero=list(range(len(first_sent_nsp_zero)))\n",
    "            logger.info('Length of indices: {}'.format(len(batch_indices_nsp_zero)))\n",
    "            logger.info('Representations(NSP Class 0) gathered. Shape of representations: {}. Length of indices: {}'.format(representations_nsp_zero.shape, len(batch_indices_nsp_zero)))\n",
    "        logger.info(f\"Representations(NSP Class 0) computed in {time.time()-representations_start_time} seconds\")\n",
    "        if accelerator.is_main_process:\n",
    "            partition_indices_nsp_zero, greedyIdx_nsp_zero, gains_nsp_zero = subset_strategy.select(len(batch_indices_nsp_zero)-1, batch_indices_nsp_zero, representations_nsp_zero, parallel_processes=args.parallel_processes, return_gains=True)\n",
    "            subset_indices_nsp_zero = [[]]\n",
    "            i=0\n",
    "            for p in gains_nsp_zero:\n",
    "                greedyList_nsp_zero.append(greedyIdx_nsp_zero[i:i+len(p)])         \n",
    "                i+=len(p)\n",
    "            probs_nsp_zero=[taylor_softmax_v1(torch.from_numpy(np.array([partition_gains])/args.temperature)).numpy()[0] for partition_gains in gains_nsp_zero]\n",
    "            rng=np.random.default_rng(args.seed+completed_steps)\n",
    "            for i, partition_prob in enumerate(probs_nsp_zero):\n",
    "                partition_budget=min(math.ceil((len(partition_prob)/len(batch_indices_nsp_zero)) * math.floor(num_samples/2)), len(partition_prob)-1)\n",
    "                subset_indices_nsp_zero[0].extend(rng.choice(greedyList_nsp_zero[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "        else:\n",
    "            subset_indices_nsp_zero=[[]]\n",
    "    \n",
    "        logger.info(f\"Performing Subset selection for NSP class 1\")\n",
    "        pbar=tqdm(range(len(first_sent_nsp_one_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "        model.eval()\n",
    "        representations_nsp_one=[]\n",
    "        batch_indices_nsp_one=[]\n",
    "        total_cnt=0\n",
    "        total_storage=0\n",
    "        representations_start_time=time.time()\n",
    "        for step, batch in enumerate(first_sent_nsp_one_dataloader):\n",
    "            with torch.no_grad():\n",
    "                output=bert_model(**batch, output_hidden_states=True)\n",
    "            embeddings=output[\"hidden_states\"][args.layer_for_similarity_computation]\n",
    "            mask=(batch['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float())\n",
    "            mask1=((batch['token_type_ids'].unsqueeze(-1).expand(embeddings.size()).float())==0)\n",
    "            mask=mask*mask1\n",
    "            mean_pooled=torch.sum(embeddings*mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "            mean_pooled = accelerator.gather(mean_pooled)\n",
    "            total_cnt += mean_pooled.size(0)\n",
    "            if accelerator.is_main_process:\n",
    "                mean_pooled = mean_pooled.cpu()\n",
    "                total_storage += sys.getsizeof(mean_pooled.storage())\n",
    "                representations_nsp_one.append(mean_pooled)\n",
    "            pbar.update(1)\n",
    "        if accelerator.is_main_process:\n",
    "            representations_nsp_one=torch.cat(representations_nsp_one, dim=0)\n",
    "            representations_nsp_one=representations_nsp_one[:len(first_sent_nsp_one)]\n",
    "            total_storage += sys.getsizeof(representations_nsp_one.storage())\n",
    "            representations_nsp_one=representations_nsp_one.numpy()\n",
    "            logger.info('Representations(NSP Class 1) Size: {}, Total number of samples: {}'.format(total_storage/(1024 * 1024), total_cnt))\n",
    "            batch_indices_nsp_one=list(range(len(first_sent_nsp_one)))\n",
    "            logger.info('Length of indices: {}'.format(len(batch_indices_nsp_one)))\n",
    "            logger.info('Representations(NSP Class 1) gathered. Shape of representations: {}. Length of indices: {}'.format(representations_nsp_one.shape, len(batch_indices_nsp_one)))\n",
    "        logger.info(f\"Representations(NSP Class 1) computed in {time.time()-representations_start_time} seconds\")\n",
    "        if accelerator.is_main_process:\n",
    "            partition_indices_nsp_one, greedyIdx_nsp_one, gains_nsp_one = subset_strategy.select(len(batch_indices_nsp_one)-1, batch_indices_nsp_one, representations_nsp_one, parallel_processes=args.parallel_processes, return_gains=True)\n",
    "            subset_indices_nsp_one = [[]]\n",
    "            i=0\n",
    "            for p in gains_nsp_one:\n",
    "                greedyList_nsp_one.append(greedyIdx_nsp_one[i:i+len(p)])         \n",
    "                i+=len(p)\n",
    "            probs_nsp_one=[taylor_softmax_v1(torch.from_numpy(np.array([partition_gains])/args.temperature)).numpy()[0] for partition_gains in gains_nsp_one]\n",
    "            rng=np.random.default_rng(args.seed+completed_steps)\n",
    "            for i, partition_prob in enumerate(probs_nsp_one):\n",
    "                partition_budget=min(math.ceil((len(partition_prob)/len(batch_indices_nsp_one)) * math.ceil(num_samples/2)), len(partition_prob)-1)\n",
    "                subset_indices_nsp_one[0].extend(rng.choice(greedyList_nsp_one[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "        else:\n",
    "            subset_indices_nsp_one=[[]]\n",
    "    accelerator.wait_for_everyone()    \n",
    "    broadcast_object_list(subset_indices_nsp_zero)\n",
    "    broadcast_object_list(subset_indices_nsp_one)\n",
    "    timing.append([0, time.time()-start_time])\n",
    "    logger.info(f\"First subset selection completed. Total Time taken(including embeddings computation): {time.time()-start_time}\")\n",
    "\n",
    "# if accelerator.is_main_process:\n",
    "#     output_file=f\"nsp_zero_subset_indices_after_step_{completed_steps}.pt\"\n",
    "#     output_file=os.path.join(args.subset_dir, output_file)\n",
    "#     torch.save(torch.tensor(subset_indices_nsp_zero), output_file)\n",
    "#     output_file=f\"nsp_one_subset_indices_after_step_{completed_steps}.pt\"\n",
    "#     output_file=os.path.join(args.subset_dir, output_file)\n",
    "#     torch.save(torch.tensor(subset_indices_nsp_one), output_file)\n",
    "#     output_file=f\"nsp_zero_gains_after_step_{completed_steps}.pkl\"\n",
    "#     output_file=os.path.join(args.subset_dir, output_file)\n",
    "#     with open(output_file, \"wb\") as f:\n",
    "#         pickle.dump(gains_nsp_zero, f)\n",
    "#     output_file=f\"nsp_one_gains_after_step_{completed_steps}.pkl\"\n",
    "#     output_file=os.path.join(args.subset_dir, output_file)\n",
    "#     with open(output_file, \"wb\") as f:\n",
    "#         pickle.dump(gains_nsp_one, f)\n",
    "#     output_file=f\"nsp_zero_partition_indices_after_step_{completed_steps}.pkl\"\n",
    "#     output_file=os.path.join(args.subset_dir, output_file)\n",
    "#     with open(output_file, \"wb\") as f:\n",
    "#         pickle.dump(partition_indices_nsp_zero, f)\n",
    "#     output_file=f\"nsp_one_partition_indices_after_step_{completed_steps}.pkl\"\n",
    "#     output_file=os.path.join(args.subset_dir, output_file)\n",
    "#     with open(output_file, \"wb\") as f:\n",
    "#         pickle.dump(partition_indices_nsp_one, f)\n",
    "#     output_file=f\"nsp_zero_greedy_indices_after_step_{completed_steps}.pkl\"\n",
    "#     output_file=os.path.join(args.subset_dir, output_file)\n",
    "#     with open(output_file, \"wb\") as f:\n",
    "#         pickle.dump(greedyIdx_nsp_zero, f)\n",
    "#     output_file=f\"nsp_one_greedy_indices_after_step_{completed_steps}.pkl\"\n",
    "#     output_file=os.path.join(args.subset_dir, output_file)\n",
    "#     with open(output_file, \"wb\") as f:\n",
    "#         pickle.dump(greedyIdx_nsp_one, f)\n",
    "\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "nsp_zero_subset_dataset=nsp_zero.select(subset_indices_nsp_zero[0])\n",
    "nsp_one_subset_dataset=nsp_one.select(subset_indices_nsp_one[0])\n",
    "# Concatenate the two datasets\n",
    "subset_dataset = concatenate_datasets([nsp_zero_subset_dataset, nsp_one_subset_dataset])\n",
    "subset_dataloader=DataLoader(\n",
    "    subset_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n",
    "subset_dataloader = accelerator.prepare(subset_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Begin the main training loop with importance re-sampling, after warm-start\")\n",
    "while completed_steps<args.max_train_steps:\n",
    "    model.train()\n",
    "    select_subset=False\n",
    "    for step, batch in enumerate(subset_dataloader):\n",
    "        train_time=0\n",
    "        subset_time=0\n",
    "        start_time=time.time()\n",
    "        outputs=model(**batch)\n",
    "        loss=outputs.loss\n",
    "        logger.info(f\"Completed Steps: {1+completed_steps}; Loss: {loss.detach().float()}; lr: {lr_scheduler.get_last_lr()};\")\n",
    "        loss=loss/args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step%args.gradient_accumulation_steps==0 or step==len(subset_dataloader)-1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps+=1\n",
    "        train_time += (time.time() - start_time)\n",
    "\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps%checkpointing_steps==0:\n",
    "                output_dir=f\"step_{completed_steps }\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir=os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "\n",
    "        if completed_steps>=args.max_train_steps:\n",
    "            break\n",
    "        \n",
    "        if (completed_steps)%args.select_every==0:\n",
    "            select_subset=True\n",
    "            break\n",
    "        timing.append([train_time, subset_time])\n",
    "    if select_subset==True:\n",
    "        accelerator.wait_for_everyone()\n",
    "        start_time = time.time()\n",
    "        num_samples = int(round(len(full_dataset) * args.subset_fraction, 0)) \n",
    "        if args.selection_strategy == 'Random-Online':\n",
    "            if accelerator.is_main_process:\n",
    "                subset_indices_nsp_zero = [random.sample(list(range(len(first_sent_nsp_zero))), math.floor(num_samples/2))]\n",
    "                subset_indices_nsp_one = [random.sample(list(range(len(first_sent_nsp_one))), math.ceil(num_samples/2))]\n",
    "            else:\n",
    "                subset_indices_nsp_zero = [[]]\n",
    "                subset_indices_nsp_one = [[]]\n",
    "        elif args.selection_strategy in [\"fl\", \"logdet\", \"gc\", \"disparity-min\"]:\n",
    "            logger.info(f\"Performing Subset selection for NSP class 0\")\n",
    "            sampling_start_time=time.time()\n",
    "            if accelerator.is_main_process:\n",
    "                subset_indices_nsp_zero=[[]]\n",
    "                rng=np.random.default_rng(args.seed+completed_steps)\n",
    "                for i, partition_prob in enumerate(probs_nsp_zero):\n",
    "                    partition_budget=min(math.ceil((len(partition_prob)/len(batch_indices_nsp_zero)) * math.floor(num_samples/2)), len(partition_prob)-1)\n",
    "                    subset_indices_nsp_zero[0].extend(rng.choice(greedyList_nsp_zero[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "            else:\n",
    "                subset_indices_nsp_zero=[[]]\n",
    "            logger.info(\"Sampling time(NSP Class 0): {}\".format(time.time()-sampling_start_time))\n",
    "        \n",
    "            logger.info(f\"Performing Subset selection for NSP class 1\")\n",
    "            sampling_start_time=time.time()\n",
    "            if accelerator.is_main_process:\n",
    "                subset_indices_nsp_one=[[]]\n",
    "                rng=np.random.default_rng(args.seed+completed_steps)\n",
    "                for i, partition_prob in enumerate(probs_nsp_one):\n",
    "                    partition_budget=min(math.ceil((len(partition_prob)/len(batch_indices_nsp_one)) * math.ceil(num_samples/2)), len(partition_prob)-1)\n",
    "                    subset_indices_nsp_one[0].extend(rng.choice(greedyList_nsp_one[i], size=partition_budget, replace=False, p=partition_prob).tolist())\n",
    "            else:\n",
    "                subset_indices_nsp_one=[[]]\n",
    "            logger.info(\"Sampling time(NSP Class 1): {}\".format(time.time()-sampling_start_time))\n",
    "        accelerator.wait_for_everyone()\n",
    "        broadcast_object_list(subset_indices_nsp_zero)\n",
    "        broadcast_object_list(subset_indices_nsp_one)\n",
    "        timing.append([0, time.time()-start_time])\n",
    "        if accelerator.is_main_process:\n",
    "            output_file=f\"nsp_zero_subset_indices_after_step_{completed_steps}.pt\"\n",
    "            output_file=os.path.join(args.subset_dir, output_file)\n",
    "            torch.save(torch.tensor(subset_indices_nsp_zero), output_file)\n",
    "            output_file=f\"nsp_one_subset_indices_after_step_{completed_steps}.pt\"\n",
    "            output_file=os.path.join(args.subset_dir, output_file)\n",
    "            torch.save(torch.tensor(subset_indices_nsp_one), output_file)\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "        nsp_zero_subset_dataset=nsp_zero.select(subset_indices_nsp_zero[0])\n",
    "        nsp_one_subset_dataset=nsp_one.select(subset_indices_nsp_one[0])\n",
    "        # Concatenate the two datasets\n",
    "        subset_dataset = concatenate_datasets([nsp_zero_subset_dataset, nsp_one_subset_dataset])\n",
    "        subset_dataloader=DataLoader(\n",
    "            subset_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n",
    "        subset_dataloader = accelerator.prepare(subset_dataloader)\n",
    "\n",
    "        subset_time=time.time()-start_time\n",
    "        select_subset=False\n",
    "        timing.append([0, subset_time])\n",
    "        logger.info(f\"Subset selection time(total resampling time): {time.time()-start_time} seconds.\")\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs=model(**batch)\n",
    "\n",
    "        loss=outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n",
    "\n",
    "    losses=torch.cat(losses)\n",
    "    losses=losses[:len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity=math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity=float(\"inf\")\n",
    "\n",
    "    logger.info(f\"Steps {completed_steps}: perplexity: {perplexity}\")\n",
    "\n",
    "logger.info(f\"Timing: {timing}\")\n",
    "logger.info(f\"Saving the final model after {completed_steps} steps.\")\n",
    "if args.output_dir is not None:\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model=accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "logger.info(f\"Training completed successfully in {time.time()-main_start_time} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
