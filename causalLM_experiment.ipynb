{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import datasets\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config, DataCollatorForLanguageModeling, DataCollatorWithPadding, get_scheduler, SchedulerType\n",
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from selectionstrategies.helper_fns import taylor_softmax_v1\n",
    "from selectionstrategies import SubmodStrategy\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler\n",
    "file_handler = logging.FileHandler('logfile.log')\n",
    "\n",
    "# Create a formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set the formatter for the file handler\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the file handler to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Log a message\n",
    "logger.info('Logger created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dataset\n",
    "ds = load_dataset(\"stas/openwebtext-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train / validation set\n",
    "\n",
    "dataset=ds[\"train\"].train_test_split(test_size=0.05, shuffle=False)\n",
    "\n",
    "dataset=datasets.DatasetDict({\"train\": dataset[\"train\"], \"validation\": dataset[\"test\"]})\n",
    "tokenizer=GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "column_names = dataset[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]   \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=None,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data\n",
    "\n",
    "block_size = 1024 # Set the length each input group to the context size\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets=tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=96,\n",
    "    load_from_cache_file=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lm_datasets['train']\n",
    "eval_dataset = lm_datasets['validation']\n",
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2Config()\n",
    "\n",
    "# Initializing a model (with random weights) from the configuration\n",
    "model = GPT2LMHeadModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_fraction = 0.25\n",
    "\n",
    "# Sample a subset of the train dataset\n",
    "num_samples = int(round(len(train_dataset) * subset_fraction, 0))\n",
    "init_subset_indices = [random.sample(list(range(len(train_dataset))), num_samples)]\n",
    "full_dataset=train_dataset\n",
    "subset_dataset = full_dataset.select(init_subset_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "batch_size = 1\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "warmstart_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=batch_size)\n",
    "full_dataloader = DataLoader(full_dataset, shuffle=False, collate_fn=data_collator, batch_size=batch_size)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=False, collate_fn=data_collator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmer = iter(warmstart_dataloader)\n",
    "X= next(warmer)\n",
    "X['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting up the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "scheduler_name = 'linear'\n",
    "num_warmup_steps = 10\n",
    "num_training_steps = 10\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# learning scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=scheduler_name,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Warmstart the model with the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmstart_epochs = 1\n",
    "completed_steps = 0\n",
    "\n",
    "model.to(device)\n",
    "# warmstart the model: Train the model with the warmstart data for warmstart epochs\n",
    "for epoch in range(warmstart_epochs):\n",
    "    if epoch==0:\n",
    "        print(\"Beginning Warmstart\")\n",
    "    model.train() # Setting the model into training mode to enable backprop \n",
    "    for step, batch in enumerate(warmstart_dataloader):\n",
    "        outputs = model(**batch.to(device))\n",
    "        loss = outputs.loss\n",
    "        print(f\"Completed Steps: {1+completed_steps}; Loss: {loss.detach().float()}; lr: {lr_scheduler.get_last_lr()};\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "        if completed_steps >= num_warmup_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_strategy = 'fl'\n",
    "num_partitions = 2000 # Default is 5000\n",
    "partition_strategy = 'random'\n",
    "submod_optimizer = 'LazyGreedy'\n",
    "\n",
    "# Define subset selection strategies\n",
    "\n",
    "subset_strategy = SubmodStrategy(logger, selection_strategy,\n",
    "    num_partitions=num_partitions, partition_strategy=partition_strategy,\n",
    "    optimizer=submod_optimizer, similarity_criterion='feature', \n",
    "    metric='cosine', eta=1, stopIfZeroGain=False, \n",
    "    stopIfNegativeGain=False, verbose=False, lambdaVal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_strategy = 'fl'\n",
    "layer_for_similarity_computation = 9\n",
    "temperature = 0.5\n",
    "seed = 23\n",
    "parallel_processes = 3\n",
    "# num_samples has already been defined when creating subset.\n",
    "probs_nsp_zero=[]\n",
    "greedyList_nsp_zero=[]\n",
    "gains_nsp_zero=[]\n",
    "\n",
    "\n",
    "# Begin subset selection for first_sent_nsp_zero\n",
    "if selection_strategy == 'Random-Online':\n",
    "    subset_indices_nsp_zero = [random.sample(list(range(len(train_dataset))), math.floor(num_samples/2))]\n",
    "elif selection_strategy in ['fl', 'logdet', 'gc', 'disparity-sum']:\n",
    "    # Choose a selection strategy\n",
    "    model.eval() # Set the model in evaluation model \n",
    "    representations=[]\n",
    "    total_cnt=0\n",
    "    total_storage=0\n",
    "    # Unwrap the model and set it in evaluation mode.\n",
    "    print(\"Performing Subset selection for entire dataset\")\n",
    "    for step, batch in enumerate(full_dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = model(**batch.to(device), output_hidden_states=True)\n",
    "        embeddings=output[\"hidden_states\"][layer_for_similarity_computation]\n",
    "        attention_mask=batch['attention_mask']\n",
    "        total_cnt+=embeddings.size(0)\n",
    "        embeddings=embeddings.cpu()\n",
    "        attention_mask=attention_mask.cpu()\n",
    "        last_token_indices=torch.cat((attention_mask, torch.zeros((embeddings.shape[0],1))), dim=1).argmin(axis=1)-1\n",
    "        embeddings=torch.cat([embeddings[i][last_token_indices[i]].reshape((1,-1)) for i in range(embeddings.shape[0])], dim=0)\n",
    "        total_storage+=sys.getsizeof(embeddings.storage())\n",
    "        representations.append(embeddings)\n",
    "        # print(f\"Current total representations: {len(representations}\")\n",
    "        \n",
    "    print(f\"Final number of representations: {len(representations)}\")\n",
    "    representations=torch.cat(representations, dim=0)\n",
    "    representations_nsp_zero=representations[:len(full_dataset)]\n",
    "    total_storage += sys.getsizeof(representations_nsp_zero.storage())\n",
    "    representations_nsp_zero=representations_nsp_zero.numpy()\n",
    "    print('Representations(NSP Class 0) Size: {}, Total number of samples: {}'.format(total_storage/(1024 * 1024), total_cnt))\n",
    "    batch_indices=list(range(len(full_dataset)))\n",
    "    print('Length of indices: {}'.format(len(batch_indices)))\n",
    "    print('Representations(NSP Class 0) gathered. Shape of representations: {}. Length of indices: {}'.format(representations_nsp_zero.shape, len(batch_indices)))\n",
    "\n",
    "    partition_indices_nsp_zero, greedyIdx_nsp_zero, gains_nsp_zero = subset_strategy.select(len(batch_indices)-1, \n",
    "                                                                                            batch_indices, representations, \n",
    "                                                                                            parallel_processes=parallel_processes, return_gains=True)\n",
    "    init_subset_indices = [[]]\n",
    "    i=0\n",
    "    for p in gains_nsp_zero:\n",
    "        greedyList_nsp_zero.append(greedyIdx_nsp_zero[i:i+len(p)])         \n",
    "        i+=len(p)\n",
    "    probs_nsp_zero=[taylor_softmax_v1(torch.from_numpy(np.array([partition_gains])/temperature)).numpy()[0] for partition_gains in gains_nsp_zero]\n",
    "    print(f\"Taylor Softmax Prop: {probs_nsp_zero}\")\n",
    "    rng=np.random.default_rng(seed+completed_steps)\n",
    "    for i, partition_prob in enumerate(probs_nsp_zero):\n",
    "        print(f\"{i}: Partition probablity :{partition_prob}\")\n",
    "        partition_budget=min(math.ceil((len(partition_prob)/len(batch_indices)) * math.floor(num_samples/2)), len(partition_prob)-1)\n",
    "        init_subset_indices[0].extend(rng.choice(greedyList_nsp_zero[i], size=partition_budget, replace=False, p=partition_prob).tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingenious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
