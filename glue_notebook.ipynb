{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import datetime\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from huggingface_hub import Repository\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")\n",
    "from transformers.utils import get_full_repo_name\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "# set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and metric\n",
    "task_name = \"mrpc\"\n",
    "train_file = None\n",
    "validation_file = None\n",
    "valid_file = None\n",
    "\n",
    "if task_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\"glue\", task_name)\n",
    "else:\n",
    "    # Loading the dataset from local csv or json file.\n",
    "    data_files = {}\n",
    "    if train_file is not None:\n",
    "        data_files[\"train\"] = train_file\n",
    "    if validation_file is not None:\n",
    "        data_files[\"validation\"] = validation_file\n",
    "    extension = (train_file if train_file is not None else valid_file).split(\".\")[-1]\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "if task_name is not None:\n",
    "    is_regression = task_name == \"stsb\"\n",
    "    if not is_regression:\n",
    "        label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "        num_labels = len(label_list)\n",
    "    else:\n",
    "        num_labels = 1\n",
    "else:\n",
    "    # Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
    "    is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "    if is_regression:\n",
    "        num_labels = 1\n",
    "    else:\n",
    "        # A useful fast method:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "        label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "        label_list.sort()  # Let's sort it for determinism\n",
    "        num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"\"\n",
    "num_labels = 2\n",
    "use_slow_tokenizer = False  \n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels, finetuning_task=task_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=not use_slow_tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "pad_to_max_length = True\n",
    "\n",
    "if task_name is not None:\n",
    "    sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "else:\n",
    "    # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
    "    non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "    if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
    "        sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
    "    else:\n",
    "        if len(non_label_column_names) >= 2:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[:2]\n",
    "        else:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[0], None\n",
    "\n",
    "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "label_to_id = None\n",
    "if (\n",
    "    model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
    "    and task_name is not None\n",
    "    and not is_regression\n",
    "):\n",
    "    # Some have all caps in their config, some don't.\n",
    "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
    "        print(\n",
    "            f\"The configuration of the model provided the following label correspondence: {label_name_to_id}. \"\n",
    "            \"Using it!\"\n",
    "        )\n",
    "        label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}\n",
    "    else:\n",
    "        print(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
    "            \"\\nIgnoring the model labels as a result.\",\n",
    "        )\n",
    "elif task_name is None:\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "if label_to_id is not None:\n",
    "    model.config.label2id = label_to_id\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "elif task_name is not None and not is_regression:\n",
    "    model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = tokenizer.model_max_length\n",
    "padding = \"max_length\" if pad_to_max_length else False\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    texts = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*texts, padding=padding, max_length=max_length, truncation=True)\n",
    "\n",
    "    if \"label\" in examples:\n",
    "        if label_to_id is not None:\n",
    "            # Map labels to IDs (not necessary for GLUE tasks)\n",
    "            result[\"labels\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
    "        else:\n",
    "            # In all cases, rename the column to labels because the model will expect that.\n",
    "            result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"validation_matched\" if task_name == \"mnli\" else \"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "\n",
    "# DataLoaders creation:\n",
    "data_collator = default_data_collator\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=per_device_train_batch_size\n",
    "    )\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_steps = 1000\n",
    "num_train_epochs = 1000\n",
    "gradient_accumulation_steps=1\n",
    "lr_scheduler_type = \"linear\"\n",
    "num_warmup_steps = 10\n",
    "\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointing_steps=1000\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "if hasattr(checkpointing_steps, \"isdigit\"):\n",
    "    checkpointing_steps = checkpointing_steps\n",
    "    if checkpointing_steps.isdigit():\n",
    "        checkpointing_steps = int(checkpointing_steps)\n",
    "else:\n",
    "    checkpointing_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metric function\n",
    "if task_name is not None:\n",
    "    metric = load_metric(\"glue\", task_name)\n",
    "else:\n",
    "    metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "total_batch_size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(train_dataset)}\")\n",
    "print(f\"  Num Epochs = {num_train_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {per_device_train_batch_size}\")\n",
    "print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "print(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {max_train_steps}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(max_train_steps))\n",
    "completed_steps = 0\n",
    "starting_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint=None\n",
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if resume_from_checkpoint:\n",
    "    if resume_from_checkpoint is not None or resume_from_checkpoint != \"\":\n",
    "        print(f\"Resumed from checkpoint: {resume_from_checkpoint}\")\n",
    "        torch.load_state(resume_from_checkpoint)\n",
    "        path = os.path.basename(resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "        dirs.sort(key=os.path.getctime)\n",
    "        path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "    # Extract `epoch_{i}` or `step_{i}`\n",
    "    training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "    if \"epoch\" in training_difference:\n",
    "        starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "        resume_step = None\n",
    "    else:\n",
    "        resume_step = int(training_difference.replace(\"step_\", \"\"))\n",
    "        starting_epoch = resume_step // len(train_dataloader)\n",
    "        resume_step -= starting_epoch * len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with_tracking=False\n",
    "push_to_hub = False\n",
    "\n",
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    model.train()\n",
    "    if with_tracking:\n",
    "        total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # We need to skip steps until we reach the resumed step\n",
    "        if resume_from_checkpoint and epoch == starting_epoch:\n",
    "            if resume_step is not None and step < resume_step:\n",
    "                completed_steps += 1\n",
    "                continue\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # We keep track of the loss at each epoch\n",
    "        if with_tracking:\n",
    "            total_loss += loss.detach().float()\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps }\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    samples_seen = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1) if not is_regression else outputs.logits.squeeze()\n",
    "        predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n",
    "        # If we are in a multiprocess environment, the last batch has duplicates\n",
    "        if accelerator.num_processes > 1:\n",
    "            if step == len(eval_dataloader):\n",
    "                predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                references = references[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            else:\n",
    "                samples_seen += references.shape[0]\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print(f\"epoch {epoch}: {eval_metric}\")\n",
    "\n",
    "    if with_tracking:\n",
    "        accelerator.log(\n",
    "            {\n",
    "                \"accuracy\" if args.task_name is not None else \"glue\": eval_metric,\n",
    "                \"train_loss\": total_loss,\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": completed_steps,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    if push_to_hub and epoch < args.num_train_epochs - 1:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "        )\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir)\n",
    "            repo.push_to_hub(\n",
    "                commit_message=f\"Training in progress epoch {epoch}\", blocking=False, auto_lfs_prune=True\n",
    "            )\n",
    "\n",
    "    if checkpointing_steps == \"epoch\":\n",
    "        output_dir = f\"epoch_{epoch}\"\n",
    "        if output_dir is not None:\n",
    "            output_dir = os.path.join(output_dir, output_dir)\n",
    "        accelerator.save_state(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_dir is not None:\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        if args.push_to_hub:\n",
    "            repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n",
    "\n",
    "if args.task_name == \"mnli\":\n",
    "    # Final evaluation on mismatched validation set\n",
    "    eval_dataset = processed_datasets[\"validation_mismatched\"]\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, collate_fn=data_collator, batch_size=per_device_eval_batch_size\n",
    "    )\n",
    "    eval_dataloader = accelerator.prepare(eval_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        metric.add_batch(\n",
    "            predictions=accelerator.gather(predictions),\n",
    "            references=accelerator.gather(batch[\"labels\"]),\n",
    "        )\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print(f\"mnli-mm: {eval_metric}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
