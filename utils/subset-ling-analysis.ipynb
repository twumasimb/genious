{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import jensenshannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/hrenduchinta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ingenious/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "# import random\n",
    "# import time\n",
    "# from tqdm.auto import tqdm\n",
    "# import submodlib\n",
    "# import faiss\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import euclidean_distances\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import faiss\n",
    "# from helper_fns import taylor_softmax_v1\n",
    "# import torch\n",
    "# import os\n",
    "# from scipy.special import softmax\n",
    "# from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 2500 of 2500]      ]21% [Iteration 525 of 2500]"
     ]
    }
   ],
   "source": [
    "# X=np.load(\"representations.npy\")\n",
    "# # X=np.load(\"wikitext-103-glove-embeddings.npy\")\n",
    "# N=X.shape[0]\n",
    "# idx=np.random.randint(0, N, size=10000).tolist()\n",
    "# representations=X[idx]\n",
    "# # faiss.normalize_L2(representations)\n",
    "# M=representations.shape[0]\n",
    "# # dist_mat=euclidean_distances(representations)\n",
    "# # data_sijs=np.exp(-dist_mat/dist_mat.mean())\n",
    "# data_sijs=cosine_similarity(representations)\n",
    "# obj = submodlib.FacilityLocationFunction(n = representations.shape[0], separate_rep=False, mode = 'dense', sijs = data_sijs)\n",
    "# greedyList=obj.maximize(budget=int(0.25*M), optimizer=\"LazyGreedy\", stopIfZeroGain=False, stopIfNegativeGain=False, verbose=False, show_progress=True)\n",
    "# subset_indices=[p[0] for p in greedyList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:14<00:00, 692.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset=load_from_disk(\"wikitext-103-processed-first-sentences\")\n",
    "# dataset=dataset[\"train\"]\n",
    "# full_partition_freq_dict={}\n",
    "# pbar=tqdm(range(len(idx)))\n",
    "# for i in idx:\n",
    "#     for id in dataset[i][\"input_ids\"]:\n",
    "#         if id in full_partition_freq_dict:\n",
    "#             full_partition_freq_dict[id]+=1\n",
    "#         else:\n",
    "#             full_partition_freq_dict[id]=1\n",
    "#     pbar.update(1)\n",
    "# with open(\"full_partition_freqs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(full_partition_freq_dict, f)\n",
    "# subset_freq_dict={}\n",
    "# pbar=tqdm(range(len(subset_indices)))\n",
    "# for i in subset_indices:\n",
    "#     for id in dataset[idx[i]][\"input_ids\"]:\n",
    "#         if id in subset_freq_dict:\n",
    "#             subset_freq_dict[id]+=1\n",
    "#         else:\n",
    "#             subset_freq_dict[id]=1\n",
    "#     pbar.update(1)\n",
    "# with open(\"subset_freqs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(subset_freq_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.txt\", \"r\") as f:\n",
    "    vocabulary=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freqs(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        d=pickle.load(f)\n",
    "        d={vocabulary[int(k)].strip():v for k, v in d.items() if ((int(k)>=1996 and int(k)<=29611) and (vocabulary[int(k)].strip() not in stop_words))}\n",
    "        d={k:v for k, v in sorted(d.items(), key=lambda x: -x[1])}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[\"fulldata-freqdict.pkl\", \"ingenious-arr.pkl\", \"lazygreedy-random.pkl\", \"full_partition_freqs.pkl\", \"subset_freqs.pkl\"]\n",
    "data=[]\n",
    "for file in files:\n",
    "    d=get_freqs(file)\n",
    "    data.append(d)\n",
    "    csv_name=f\"{file[:-4]}.csv\"\n",
    "    with open(csv_name, \"w\") as f:\n",
    "        f.write(\"word,count\\n\")\n",
    "        for k, v in d.items():\n",
    "            f.write(f\"{k},{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame.from_dict(data[:3]).transpose().fillna(0)\n",
    "df.columns=[\"full-data\", \"ingenious-arr\", \"lazygreedy\"]\n",
    "for col in df.columns:\n",
    "    new_name=f\"{col}-frac\"\n",
    "    df[new_name]=df[col]/df[col].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    1.070555e-02\n",
       "##s         7.760591e-03\n",
       "one         3.030728e-03\n",
       "also        2.984038e-03\n",
       "first       2.658172e-03\n",
       "                ...     \n",
       "##₀         1.738315e-08\n",
       "##₄         1.587157e-08\n",
       "h₂o         1.209262e-08\n",
       "##ᵢ         6.802100e-09\n",
       "##ₙ         1.511578e-09\n",
       "Name: full-data-frac, Length: 27471, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"full-data-frac\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003150739504721935"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensenshannon(df[\"full-data-frac\"], df[\"ingenious-arr-frac\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingenious",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "330e453a0b5d4a31de1248b81498e4f7229811b6cdc584e51aea2376965a21af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
